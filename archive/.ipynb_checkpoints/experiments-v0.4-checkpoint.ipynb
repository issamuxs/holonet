{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6abcbc-cf38-4fdd-a3fa-26c1a381e44e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T07:42:19.042133Z",
     "iopub.status.busy": "2025-07-01T07:42:19.041422Z",
     "iopub.status.idle": "2025-07-01T07:42:21.732524Z",
     "shell.execute_reply": "2025-07-01T07:42:21.732064Z",
     "shell.execute_reply.started": "2025-07-01T07:42:19.042110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->thop) (1.3.0)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e2047a-91d2-4497-8f30-2bb2130652da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:54:21.265629Z",
     "iopub.status.busy": "2025-06-30T16:54:21.265382Z",
     "iopub.status.idle": "2025-06-30T16:54:21.579170Z",
     "shell.execute_reply": "2025-06-30T16:54:21.578526Z",
     "shell.execute_reply.started": "2025-06-30T16:54:21.265611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from thop import profile\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    module=\"scipy.optimize._minpack_py\"\n",
    ")\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# === Utils ===\n",
    "\n",
    "_ds_cache = {}\n",
    "\n",
    "def get_data(ds_name: str, resize: int):\n",
    "    \"\"\"\n",
    "    Return a torchvision Dataset for `ds_name` ('cifar' or 'mnist'),\n",
    "    resized to (resize × resize).  Downloads only on first call.\n",
    "    \"\"\"\n",
    "    key = (ds_name, resize)\n",
    "    if key not in _ds_cache:\n",
    "        # pick the right class\n",
    "        DataClass = datasets.CIFAR10 if ds_name == 'cifar' else datasets.MNIST\n",
    "\n",
    "        # build & stash it\n",
    "        tf = transforms.Compose([\n",
    "            transforms.Resize((resize, resize)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        _ds_cache[key] = DataClass(\n",
    "            './data',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=tf\n",
    "        )\n",
    "    return _ds_cache[key]\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def summarize_sd(arr):\n",
    "    arr = np.asarray(arr, float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return \"n/a\"\n",
    "    return f\"{arr.mean():.3f} ± {arr.std():.3f} (SD, n={len(arr)})\"\n",
    "\n",
    "def summarize_ci(arr, alpha=0.05):\n",
    "    \"\"\"\n",
    "    mean ± 95 % CI (Student-t, two-sided).\n",
    "    Returns 'n/a' if nothing to summarise.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    n   = len(arr)\n",
    "    if n == 0:\n",
    "        return \"n/a\"\n",
    "    mean = arr.mean()\n",
    "    sem  = stats.sem(arr)               \n",
    "    half = stats.t.ppf(1-alpha/2, n-1) * sem\n",
    "    return f\"{mean:.3f} ± {half:.3f} (95 % CI, n={n})\"\n",
    "\n",
    "def summarize_p(arr):\n",
    "    \"\"\"median [IQR] for p-values – unchanged.\"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return \"n/a\"\n",
    "    q25, q75 = np.percentile(arr, [25, 75])\n",
    "    return f\"{np.median(arr):.3f} [IQR {q25:.3f}–{q75:.3f}] (n={len(arr)})\"\n",
    "\n",
    "# === Models ===\n",
    "class ModularCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_channels,\n",
    "        kernel_sizes,\n",
    "        strides,\n",
    "        use_leaky_relu: bool = False,\n",
    "        use_batchnorm: bool = False,\n",
    "        in_channels: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList() if use_batchnorm else None\n",
    "        self.use_bn = use_batchnorm\n",
    "        self.strides = strides\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.use_leaky = use_leaky_relu\n",
    "        for out_ch, k, s in zip(conv_channels, kernel_sizes, strides):\n",
    "            conv = nn.Conv2d(in_channels, out_ch, kernel_size=k, padding=k//2, bias=True)\n",
    "            self.convs.append(conv)\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm2d(out_ch))\n",
    "            in_channels = out_ch\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[idx](x)\n",
    "            x = F.leaky_relu(x, 0.01) if self.use_leaky else F.relu(x)\n",
    "            activations.append(x.clone())\n",
    "            s = self.strides[idx]\n",
    "            x = F.avg_pool2d(x, kernel_size=s)\n",
    "        return activations\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        out_ch: int,\n",
    "        stride: int = 1,\n",
    "        kernel_size: int = 3,\n",
    "        bottleneck_ratio: float = 1.0,\n",
    "        projection_type: str = 'identity',\n",
    "        activation: str = 'relu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mid_ch = int(out_ch * bottleneck_ratio)\n",
    "        # 1st conv\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(mid_ch)\n",
    "        # 2nd conv\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        # shortcut\n",
    "        if projection_type == 'conv1x1' or stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        # activation\n",
    "        if activation == 'leaky_relu':\n",
    "            self.act = lambda x: F.leaky_relu(x, 0.01)\n",
    "        else:\n",
    "            self.act = F.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y = y + self.shortcut(x)\n",
    "        return self.act(y)\n",
    "\n",
    "class ModularResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        block_sizes,\n",
    "        kernel_sizes,\n",
    "        strides,\n",
    "        bottleneck_ratios,\n",
    "        projection_types,\n",
    "        activation_functions,\n",
    "        in_channels: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # stem\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels[0])\n",
    "        self.act1  = F.relu\n",
    "        self.in_ch = channels[0]\n",
    "        # store params\n",
    "        self.channels = channels\n",
    "        self.block_sizes = block_sizes\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.strides_list = strides\n",
    "        self.bottleneck_ratios = bottleneck_ratios\n",
    "        self.projection_types = projection_types\n",
    "        self.activation_functions = activation_functions\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(len(channels)):\n",
    "            self.layers.append(self._make_layer(\n",
    "                out_ch = channels[idx],\n",
    "                num_blocks = block_sizes[idx],\n",
    "                stride = strides[idx],\n",
    "                kernel_size = kernel_sizes[idx],\n",
    "                bottleneck_ratio = bottleneck_ratios[idx],\n",
    "                projection_type = projection_types[idx],\n",
    "                activation = activation_functions[idx]\n",
    "            ))\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        out_ch,\n",
    "        num_blocks,\n",
    "        stride,\n",
    "        kernel_size,\n",
    "        bottleneck_ratio,\n",
    "        projection_type,\n",
    "        activation\n",
    "    ):\n",
    "        blocks = []\n",
    "        # first block\n",
    "        blocks.append(ResNetBlock(\n",
    "            self.in_ch, out_ch, stride,\n",
    "            kernel_size, bottleneck_ratio,\n",
    "            projection_type, activation\n",
    "        ))\n",
    "        self.in_ch = out_ch\n",
    "        # rest\n",
    "        for _ in range(1, num_blocks):\n",
    "            blocks.append(ResNetBlock(\n",
    "                self.in_ch, out_ch, 1,\n",
    "                kernel_size, bottleneck_ratio,\n",
    "                'identity', activation\n",
    "            ))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        x = self.act1(self.bn1(self.conv1(x)))\n",
    "        acts.append(x.clone())\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            acts.append(x.clone())\n",
    "        return acts\n",
    "\n",
    "# === Architecture Generators ===\n",
    "def generate_random_cnn(\n",
    "    n_layers: int,\n",
    "    stride_choices,\n",
    "    kernel_choices,\n",
    "    channel_choices,\n",
    "    input_size: int,\n",
    "    in_channels: int,\n",
    "    use_leaky_relu: bool,\n",
    "    use_batchnorm: bool\n",
    "):\n",
    "    size = input_size\n",
    "    channels, kernels, strides = [], [], []\n",
    "    for _ in range(n_layers):\n",
    "        s = random.choice([st for st in stride_choices if size >= 2*st] or [1])\n",
    "        k = random.choice(kernel_choices)\n",
    "        c = random.choice(channel_choices)\n",
    "        strides.append(s)\n",
    "        kernels.append(k)\n",
    "        channels.append(c)\n",
    "        size = max(1, size // s)\n",
    "    return ModularCNN(channels, kernels, strides, use_leaky_relu, use_batchnorm, in_channels)\n",
    "\n",
    "def generate_random_resnet(\n",
    "    n_layers: int,\n",
    "    channel_choices,\n",
    "    input_size: int,\n",
    "    in_channels: int,\n",
    "    block_sizes,\n",
    "    kernel_sizes,\n",
    "    stride_choices,\n",
    "    bottleneck_ratios,\n",
    "    projection_types,\n",
    "    activation_functions\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a random ResNet with n_layers blocks. For each block we sample:\n",
    "      - out_channels from channel_choices\n",
    "      - num_blocks from block_sizes\n",
    "      - kernel_size from kernel_sizes\n",
    "      - stride from stride_choices (only if spatial size allows)\n",
    "      - bottleneck_ratio from bottleneck_ratios\n",
    "      - projection_type from projection_types\n",
    "      - activation from activation_functions\n",
    "    \"\"\"\n",
    "    size = input_size\n",
    "    channels, blocks, ks, strides, brs, pts, afs = [], [], [], [], [], [], []\n",
    "\n",
    "    # Sample up to n_layers (stop early if spatial collapses)\n",
    "    for _ in range(n_layers):\n",
    "        c  = random.choice(channel_choices)\n",
    "        b  = random.choice(block_sizes)\n",
    "        k  = random.choice(kernel_sizes)\n",
    "        br = random.choice(bottleneck_ratios)\n",
    "        pt = random.choice(projection_types)\n",
    "        af = random.choice(activation_functions)\n",
    "\n",
    "        # only allow stride>1 if size>=2*stride\n",
    "        possible_strides = [s for s in stride_choices if size >= 2*s]\n",
    "        s = random.choice(possible_strides) if possible_strides else 1\n",
    "\n",
    "        channels.append(c)\n",
    "        blocks.append(b)\n",
    "        ks.append(k)\n",
    "        strides.append(s)\n",
    "        brs.append(br)\n",
    "        pts.append(pt)\n",
    "        afs.append(af)\n",
    "\n",
    "        size = max(1, size // s)\n",
    "        if size <= 1:\n",
    "            break\n",
    "\n",
    "    return ModularResNet(\n",
    "        channels       = channels,\n",
    "        block_sizes    = blocks,\n",
    "        kernel_sizes   = ks,\n",
    "        strides        = strides,\n",
    "        bottleneck_ratios   = brs,\n",
    "        projection_types    = pts,\n",
    "        activation_functions= afs,\n",
    "        in_channels    = in_channels\n",
    "    )\n",
    "\n",
    "# === Property Extraction ===\n",
    "def extract_cnn_properties(model: ModularCNN):\n",
    "    \"\"\"\n",
    "    Extracts layer-wise properties for a ModularCNN.\n",
    "    Uses naming consistent with ResNet properties for shared concepts.\n",
    "    \"\"\"\n",
    "    strides, kernels, channels, depths, overlaps = [], [], [], [], []\n",
    "    for i, conv in enumerate(model.convs):\n",
    "        k = model.kernel_sizes[i]\n",
    "        s = model.strides[i]\n",
    "        c = conv.out_channels\n",
    "        strides.append(s)\n",
    "        kernels.append(k)\n",
    "        channels.append(c)\n",
    "        depths.append(i+1)\n",
    "        overlaps.append((k - s) / k)\n",
    "\n",
    "    # Use 'kernel_size' and 'stride' to be consistent with ResNet extractor\n",
    "    return {\n",
    "        'stride': np.array(strides),\n",
    "        'kernel_size': np.array(kernels),\n",
    "        'c': np.array(channels),\n",
    "        'inv_c': 1/np.array(channels),\n",
    "        'd': np.array(depths),\n",
    "        'o': np.array(overlaps),\n",
    "        's_r': np.array([s*r for s,r in zip(strides, kernels)]),\n",
    "        'arch_type': np.full_like(np.array(depths), 'cnn', dtype=object)\n",
    "    }\n",
    "\n",
    "def extract_resnet_properties(model: ModularResNet):\n",
    "    ks = model.kernel_sizes\n",
    "    ss = model.strides_list\n",
    "    br = model.bottleneck_ratios\n",
    "    pt = model.projection_types\n",
    "    af = model.activation_functions\n",
    "    skip_width, block_depth, layer_index, channels, res_type = [], [], [], [], []\n",
    "    kernel_size, stride, bottleneck, projection, activation, depths = [], [], [], [], [], []\n",
    "\n",
    "    # initial conv\n",
    "    layer = 1\n",
    "    channels.append(model.conv1.out_channels)\n",
    "    skip_width.append(0)\n",
    "    block_depth.append(0)\n",
    "    layer_index.append(layer)\n",
    "    res_type.append(0)\n",
    "    kernel_size.append(3)\n",
    "    stride.append(1)\n",
    "    bottleneck.append(1.0)\n",
    "    projection.append(0)\n",
    "    activation.append(0)\n",
    "    depths.append(layer)\n",
    "    layer += 1\n",
    "\n",
    "    for li, seq in enumerate(model.layers):\n",
    "        for bj, blk in enumerate(seq):\n",
    "            in_ch = blk.conv1.in_channels\n",
    "            out_ch = blk.conv2.out_channels\n",
    "            has_proj = not isinstance(blk.shortcut, nn.Identity)\n",
    "\n",
    "            skip_width.append(out_ch - in_ch if has_proj else 0)\n",
    "            block_depth.append(bj)\n",
    "            layer_index.append(layer)\n",
    "            res_type.append(1 if has_proj else 0)\n",
    "\n",
    "            if bj == 0:\n",
    "                kernel_size.append(ks[li])\n",
    "                stride.append(ss[li])\n",
    "                bottleneck.append(br[li])\n",
    "                projection.append(1 if pt[li] != 'identity' else 0)\n",
    "                activation.append(1 if af[li]=='leaky_relu' else 0)\n",
    "            else:\n",
    "                kernel_size.append(ks[li])\n",
    "                stride.append(1)\n",
    "                bottleneck.append(br[li])\n",
    "                projection.append(0)\n",
    "                activation.append(1 if af[li]=='leaky_relu' else 0)\n",
    "\n",
    "            channels.append(out_ch)\n",
    "            depths.append(layer)\n",
    "            layer += 1\n",
    "\n",
    "    return {\n",
    "        'skip_width': np.array(skip_width),\n",
    "        'block_depth': np.array(block_depth),\n",
    "        'layer_index': np.array(layer_index),\n",
    "        'c': np.array(channels),\n",
    "        'inv_c': 1/np.array(channels),\n",
    "        'res_type': np.array(res_type),\n",
    "        'kernel_size': np.array(kernel_size),\n",
    "        'stride': np.array(stride),\n",
    "        'bottleneck': np.array(bottleneck),\n",
    "        'projection': np.array(projection),\n",
    "        'activation': np.array(activation),\n",
    "        'd': np.array(depths),\n",
    "        'arch_type': np.full_like(np.array(depths), 'resnet', dtype=object)\n",
    "    }\n",
    "\n",
    "def extract_model_properties(model):\n",
    "    if isinstance(model, ModularCNN):\n",
    "        return extract_cnn_properties(model)\n",
    "    elif isinstance(model, ModularResNet):\n",
    "        return extract_resnet_properties(model)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "        \n",
    "def extract_natural_features(model, raw_props_df: pd.DataFrame, input_shape: tuple) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates a rich, \"natural\" set of features for a given model,\n",
    "    combining global budget, shape, and architecture-specific properties.\n",
    "    Now uses MACs for computational budget.\n",
    "\n",
    "    Args:\n",
    "        model: The instantiated PyTorch model object.\n",
    "        raw_props_df: The DataFrame containing the layer-wise properties for this single model.\n",
    "        input_shape: A tuple representing the input shape (e.g., (1, 3, 44, 44)) for MACs calculation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the engineered features for this one model.\n",
    "    \"\"\"\n",
    "    # --- Basic Properties ---\n",
    "    total_depth = len(raw_props_df)\n",
    "    arch_type = raw_props_df['arch_type'].iloc[0]\n",
    "    \n",
    "    # --- 1. Computational Budget Features (Using MACs) ---\n",
    "    # Create a dummy input tensor with the correct shape to profile the model\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    \n",
    "    # Use thop to calculate MACs and params. model must be on CPU for thop.\n",
    "    macs, params = profile(model.to('cpu'), inputs=(dummy_input,), verbose=False)\n",
    "    \n",
    "    features = {\n",
    "        # --- THIS IS THE KEY CHANGE ---\n",
    "        'total_macs_log': np.log1p(macs),\n",
    "        'param_density': params / total_depth # Keep param_density as it's still interesting\n",
    "    }\n",
    "    \n",
    "    # --- 2. Global Shape (\"Pyramid\") Features ---\n",
    "    channels = raw_props_df['c']\n",
    "    strides = raw_props_df['stride']\n",
    "    \n",
    "    features['initial_channels'] = channels.iloc[0]\n",
    "    features['max_channels'] = channels.max()\n",
    "    features['width_expansion_factor'] = features['max_channels'] / (features['initial_channels'] + 1e-8)\n",
    "    \n",
    "    downsampling_stages = (strides > 1).sum()\n",
    "    features['downsampling_stages'] = downsampling_stages\n",
    "    if downsampling_stages > 0:\n",
    "        features['layers_per_stage'] = total_depth / downsampling_stages\n",
    "    else:\n",
    "        features['layers_per_stage'] = total_depth\n",
    "\n",
    "    # --- 3. Flow Consistency Features ---\n",
    "    channel_ratio = channels / channels.shift(1).fillna(channels.iloc[0])\n",
    "    features['mean_channel_ratio'] = channel_ratio.mean()\n",
    "    features['std_channel_ratio'] = channel_ratio.std()\n",
    "    features['mean_stride'] = strides.mean()\n",
    "    \n",
    "    # --- 4. Architecture-Specific Features ---\n",
    "    if arch_type == 'resnet':\n",
    "        if 'projection' in raw_props_df and 'res_type' in raw_props_df:\n",
    "            num_projections = raw_props_df['projection'].sum()\n",
    "            num_blocks = (raw_props_df['res_type'] > 0).sum()\n",
    "            num_identities = num_blocks - num_projections\n",
    "            \n",
    "            features['num_projection_shortcuts'] = num_projections\n",
    "            features['num_identity_shortcuts'] = num_identities\n",
    "            if num_blocks > 0:\n",
    "                features['identity_shortcut_ratio'] = num_identities / num_blocks\n",
    "            else:\n",
    "                features['identity_shortcut_ratio'] = 0\n",
    "            features['mean_bottleneck_ratio'] = raw_props_df['bottleneck'].mean()\n",
    "        \n",
    "    elif arch_type == 'cnn':\n",
    "        if 'o' in raw_props_df:\n",
    "            features['mean_overlap'] = raw_props_df['o'].mean()\n",
    "\n",
    "    return features\n",
    "\n",
    "# === Metrics ===\n",
    "\n",
    "def compute_robust_lambda(model, x, device, num_noise_steps=10, max_noise_std=0.3, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Computes a robust, intrinsic lambda by fitting a sensitivity curve for each layer.\n",
    "    It measures the amplification of perturbations across a spectrum of noise levels.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        x: The input tensor.\n",
    "        device: The device to run on.\n",
    "        num_noise_steps: The number of noise levels to test.\n",
    "        max_noise_std: The maximum noise standard deviation to test.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of robust lambda values for the model.\n",
    "    \"\"\"\n",
    "    noise_levels_to_test = np.linspace(0.01, max_noise_std, num_noise_steps)\n",
    "    all_deltas = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        clean_acts = model(x)\n",
    "        if isinstance(clean_acts, tuple):\n",
    "            clean_acts = clean_acts[0]\n",
    "\n",
    "        for noise_std in noise_levels_to_test:\n",
    "            noisy_input = x + noise_std * torch.randn_like(x)\n",
    "            noisy_acts = model(noisy_input)\n",
    "            if isinstance(noisy_acts, tuple):\n",
    "                noisy_acts = noisy_acts[0]\n",
    "\n",
    "            deltas_for_level = torch.stack(\n",
    "                [(n_act - c_act).flatten(1).norm(dim=1) for n_act, c_act in zip(noisy_acts, clean_acts)]\n",
    "            ).mean(dim=1).cpu().numpy()\n",
    "            all_deltas.append(deltas_for_level)\n",
    "    \n",
    "    all_deltas = np.array(all_deltas)\n",
    "    num_layers = all_deltas.shape[1]\n",
    "    robust_lambdas = []\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        x_data = all_deltas[:, i]\n",
    "        y_data = all_deltas[:, i+1]\n",
    "        \n",
    "        try:\n",
    "            # Fit a line: y = m*x. The slope 'm' is the amplification factor.\n",
    "            # We force the intercept through zero, which is physically motivated.\n",
    "            x_data_reshaped = x_data[:, np.newaxis]\n",
    "            slope, _, _, _ = np.linalg.lstsq(x_data_reshaped, y_data, rcond=None)\n",
    "            slope = slope[0]\n",
    "        except np.linalg.LinAlgError:\n",
    "            slope = 1.0\n",
    "\n",
    "        amplification_factor = max(slope, eps)\n",
    "        robust_lambdas.append(-np.log(amplification_factor))\n",
    "        \n",
    "    return np.array(robust_lambdas)\n",
    "\n",
    "\n",
    "def compute_input_sensitivity(model, x):\n",
    "    model.eval()\n",
    "    xv = x.clone().requires_grad_(True)\n",
    "    acts = model(xv)\n",
    "\n",
    "    input_sens = []\n",
    "    for act in acts:\n",
    "        # exactly the same mathematical expression as before\n",
    "        loss = act.square().sum()\n",
    "        (grad,) = torch.autograd.grad(loss, xv, retain_graph=True)\n",
    "        input_sens.append(grad.norm().item())\n",
    "\n",
    "    return input_sens\n",
    "\n",
    "def mi_permutation_test(X, y, n_perm=200):\n",
    "    n_neighbors = max(1, min(2, len(y) - 1))  \n",
    "    actual_mi = mutual_info_regression(X, y, random_state=0, n_neighbors=n_neighbors)[0]\n",
    "    perm_mis = []\n",
    "    for _ in range(n_perm):\n",
    "        y_perm = np.random.permutation(y)\n",
    "        perm_mi = mutual_info_regression(X, y_perm, random_state=0, n_neighbors=n_neighbors)[0]\n",
    "        perm_mis.append(perm_mi)\n",
    "    pval = (np.sum(np.array(perm_mis) >= actual_mi) + 1) / (n_perm + 1)\n",
    "    return actual_mi, pval\n",
    "\n",
    "# === Regression & Analysis ===\n",
    "\n",
    "def plot_key_results(results, title_suffix=\"\"):\n",
    "    # 1) Input Sensitivity vs Depth\n",
    "    grouped = results['grouped']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(grouped['depth'], grouped['mean'], yerr=grouped['std'], fmt='o-', capsize=3)\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.ylabel(\"Mean Input Sensitivity\")\n",
    "    plt.title(f\"Input Sensitivity vs Depth{title_suffix}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2) σ² vs 1/(L–1) Regression\n",
    "    inv   = results['inv_Lminus1']      # array of 1/(L_i - 1)\n",
    "    sigma2 = results['sigma2']          # array of σ_i²\n",
    "    slope, intercept, r_value, p_value, stderr = linregress(inv, sigma2)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(inv, sigma2, alpha=0.5, label=\"data\")\n",
    "    xs = np.linspace(inv.min(), inv.max(), 200)\n",
    "    plt.plot(xs, intercept + slope*xs, 'r--',\n",
    "             label=f\"fit: y={slope:.2e}x+{intercept:.2e}\\n$R^2$={r_value**2:.2f}\")\n",
    "    plt.xlabel(\"1 / (L - 1)\")\n",
    "    plt.ylabel(\"σ²\")\n",
    "    plt.title(f\"σ² vs 1/(L–1){title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_global_complexity(raw_props, n_components=1):\n",
    "    \"\"\"\n",
    "    Compute a 1D \"ACS\" per model as the projection onto the first principal component\n",
    "    of the global (minmax‐scaled) parameter space.\n",
    "    \"\"\"\n",
    "    # --- THIS IS THE FIX ---\n",
    "    # 1) Get keys for numeric columns only, filtering out strings like 'arch_type'.\n",
    "    numeric_keys = [k for k, v in raw_props.items() if np.issubdtype(v.dtype, np.number)]\n",
    "    if not numeric_keys:\n",
    "        # If no numeric columns are found, return an array of zeros.\n",
    "        num_models = len(next(iter(raw_props.values())))\n",
    "        return np.zeros(num_models)\n",
    "        \n",
    "    # Stack parameters into (n_models, n_features) using only numeric keys\n",
    "    X = np.vstack([ raw_props[k] for k in numeric_keys ]).T\n",
    "\n",
    "    # 2) Min–max scale each column to [0,1]\n",
    "    # Add a small epsilon to the denominator to prevent division by zero\n",
    "    # if a feature is constant across all models.\n",
    "    mins = X.min(axis=0, keepdims=True)\n",
    "    maxs = X.max(axis=0, keepdims=True)\n",
    "    X_norm = (X - mins) / (maxs - mins + 1e-8)\n",
    "\n",
    "    # 3) PCA to 1 component\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(X_norm)  # shape (n_models, 1)\n",
    "\n",
    "    # 4) Return the first principal component (flattened)\n",
    "    return pcs[:, 0]\n",
    "\n",
    "def classify_models(complexity_scores, percentile=10):\n",
    "    sorted_idx = np.argsort(complexity_scores)\n",
    "    n = len(complexity_scores)\n",
    "    labels = np.array(['unlabeled'] * n)\n",
    "    k = max(1, int(n * percentile / 100))\n",
    "    labels[sorted_idx[:k]] = 'simple'\n",
    "    labels[sorted_idx[-k:]] = 'complex'\n",
    "    return labels\n",
    "\n",
    "def bin_and_validate(input_sens_means, complexity_labels, n_bins=10):\n",
    "    bins = np.linspace(min(input_sens_means), max(input_sens_means), n_bins + 1)\n",
    "    bin_indices = np.digitize(input_sens_means, bins) - 1\n",
    "    valid_bins = []\n",
    "    for bin_id in range(n_bins):\n",
    "        idx = np.where(bin_indices == bin_id)[0]\n",
    "        labels_in_bin = complexity_labels[idx]\n",
    "        simple_count = np.sum(labels_in_bin == 'simple')\n",
    "        complex_count = np.sum(labels_in_bin == 'complex')\n",
    "        if len(idx) >= 5 and simple_count >= 3 and complex_count >= 3:\n",
    "            valid_bins.append({\n",
    "                'indices': idx,\n",
    "                'simple_count': simple_count,\n",
    "                'complex_count': complex_count,\n",
    "                'bin_id': bin_id\n",
    "            })\n",
    "    return valid_bins\n",
    "\n",
    "def compute_bin_metrics(valid_bins, lambda_vals, complexity_scores, raw_props, complexity_labels):\n",
    "    \"\"\"\n",
    "    For each valid FI‐bin, compute:\n",
    "      - CV of σ\n",
    "      - Mutual Information on ACS and on all_params (with p‐values)\n",
    "      - σ(simple)/σ(complex) ratio and its KS‐test p‐value\n",
    "    Returns a pandas.DataFrame with columns:\n",
    "      ['IS_bin', '#simple', '#complex', 'CV',\n",
    "       'ratio', 'pval_ratio',\n",
    "       'MI_ACS', 'MI_pval_ACS',\n",
    "       'MI_all_params', 'MI_pval_all_params']\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Filter for numeric keys once, outside the loop.\n",
    "    numeric_keys = [k for k, v in raw_props.items() if np.issubdtype(v.dtype, np.number)]\n",
    "    \n",
    "    for b in valid_bins:\n",
    "        idx = b['indices']\n",
    "\n",
    "        # compute σ for all models in this bin\n",
    "        sigma_l = np.array([np.std(lambda_vals[i]) for i in idx])\n",
    "\n",
    "        # coefficient of variation\n",
    "        CV = sigma_l.std() / (sigma_l.mean() + 1e-12)\n",
    "\n",
    "        # --- simple vs complex σ ---\n",
    "        lam_s = [np.std(lambda_vals[i]) for i in idx if complexity_labels[i] == 'simple']\n",
    "        lam_c = [np.std(lambda_vals[i]) for i in idx if complexity_labels[i] == 'complex']\n",
    "        # KS‐test on the two distributions\n",
    "        _, p_val = ks_2samp(lam_s, lam_c)\n",
    "        # ratio of means\n",
    "        ratio = np.nanmean(lam_s) / np.nanmean(lam_c)\n",
    "\n",
    "        # mutual information tests\n",
    "        acs         = complexity_scores[idx].reshape(-1, 1)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Build all_params using only the pre-filtered numeric keys.\n",
    "        all_params  = np.array([[raw_props[k][i] for k in numeric_keys] for i in idx])\n",
    "        \n",
    "        mi_acs, p_acs       = mi_permutation_test(acs, sigma_l)\n",
    "        mi_params, p_params = mi_permutation_test(all_params, sigma_l)\n",
    "\n",
    "        metrics.append({\n",
    "            'IS_bin':            b['bin_id'],\n",
    "            '#simple':           b['simple_count'],\n",
    "            '#complex':          b['complex_count'],\n",
    "            'CV':                CV,\n",
    "            'ratio':             ratio,\n",
    "            'pval_ratio':        p_val,\n",
    "            'MI_ACS':            mi_acs,\n",
    "            'MI_pval_ACS':       p_acs,\n",
    "            'MI_all_params':     mi_params,\n",
    "            'MI_pval_all_params': p_params\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def train_and_analyze_stability_model(X: pd.DataFrame, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Trains a Gradient Boosting model to predict sigma (RG flow stability)\n",
    "    from engineered architectural features and returns feature importances.\n",
    "\n",
    "    Args:\n",
    "        X: DataFrame of engineered features (one row per model).\n",
    "        y: NumPy array of sigma values (std(lambda_i)) for each model.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - A DataFrame of sorted feature importances.\n",
    "        - The mean cross-validated R^2 score of the model.\n",
    "    \"\"\"\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(\"Mismatch between number of models in X and y.\")\n",
    "    if len(X) < 10: # Not enough data to train a meaningful model\n",
    "        return pd.DataFrame(), np.nan\n",
    "\n",
    "    # Define the model\n",
    "    gbr = GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=0)\n",
    "\n",
    "    # Calculate cross-validated R^2 score to see how well we can predict sigma\n",
    "    try:\n",
    "        cv_scores = cross_val_score(gbr, X, y, cv=min(5, len(X)), scoring='r2')\n",
    "        mean_sigma_r2 = np.mean(cv_scores)\n",
    "    except Exception:\n",
    "        mean_sigma_r2 = np.nan\n",
    "\n",
    "    # Train the model on all data to get final feature importances\n",
    "    gbr.fit(X, y)\n",
    "\n",
    "    # Create a DataFrame of feature importances\n",
    "    importances = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': gbr.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return importances, mean_sigma_r2\n",
    "\n",
    "# === Experiment Runner ===\n",
    "def run_experiment(cfg):\n",
    "    # ───────────────────────────── SET-UP ──────────────────────────────\n",
    "    set_random_seed(cfg['seed'])\n",
    "    ds = cfg['dataset'].lower()\n",
    "    base, ch = (32, 3) if ds == 'cifar' else (28, 1)\n",
    "    scale = random.uniform(*cfg['resize_range'])\n",
    "    tgt = int(base * scale)\n",
    "\n",
    "    print(f\"Running run_id       : {cfg['run_id']}\")\n",
    "    print(f\"\\nResize scale factor  : {scale:.2f}  →  target size {tgt}\")\n",
    "\n",
    "    data = get_data(ds, tgt)\n",
    "    n = max(1, int(len(data) * cfg['data_fraction']))\n",
    "    idx = np.random.choice(len(data), n, replace=False)\n",
    "    samples = [data[i][0] for i in idx]\n",
    "    x = torch.stack(samples, dim=0).to(device)\n",
    "    # noise_std is now IGNORED by the lambda calculation but kept for compatibility\n",
    "    input_size, in_channels = tgt, ch\n",
    "\n",
    "    print(f\"Input shape          : {tuple(x.shape)}\")\n",
    "\n",
    "    # ───────────────────── GENERATE & MEASURE MODELS ────────────────────\n",
    "    data_pts, input_sens_vals, lambda_vals = [], [], []\n",
    "    lambda_stds, model_depths          = [], []\n",
    "\n",
    "    all_natural_features = []\n",
    "\n",
    "    def _measure_model(model):\n",
    "        \n",
    "        lam = compute_robust_lambda(model, x, device)\n",
    "\n",
    "        if lam.size == 0 or len(lam) < 3:\n",
    "            return None\n",
    "        with autocast(enabled=False):\n",
    "            input_sens = compute_input_sensitivity(model, x)\n",
    "        props = extract_model_properties(model)\n",
    "        ml = min(len(lam)+1, len(input_sens))\n",
    "        for k in props:\n",
    "            props[k] = props[k][:ml]\n",
    "        rows = []\n",
    "        for j in range(ml-1):\n",
    "            rows.append({**{k: props[k][j] for k in props}, 'empirical': float(lam[j])})\n",
    "        return lam, input_sens, rows, props\n",
    "\n",
    "    model_id_counter = 0\n",
    "    for _ in tqdm(range(cfg['n_models']), desc=\"Measuring models\"):\n",
    "        depth = random.choice(cfg['depth_choices'])\n",
    "        if cfg['architecture'] == 'cnn':\n",
    "            model = generate_random_cnn(\n",
    "                depth,\n",
    "                cfg['param_ranges']['strides'],\n",
    "                cfg['param_ranges']['kernels'],\n",
    "                cfg['param_ranges']['channels'],\n",
    "                input_size, in_channels,\n",
    "                cfg.get('use_leaky_relu', True),\n",
    "                cfg.get('use_batchnorm', False)\n",
    "            ).to(device)\n",
    "        else:\n",
    "            pr = cfg['param_ranges']\n",
    "            model = generate_random_resnet(\n",
    "                depth, pr['channels'], input_size, in_channels,\n",
    "                pr['block_sizes'], pr['kernel_sizes'], pr['strides'],\n",
    "                pr['bottleneck_ratios'], pr['projection_types'],\n",
    "                pr['activation_functions']\n",
    "            ).to(device)\n",
    "        \n",
    "        res = _measure_model(model)\n",
    "        \n",
    "        if res is None:\n",
    "            del model\n",
    "            continue\n",
    "            \n",
    "        lam, input_sens, rows, raw_props_dict = res\n",
    "        \n",
    "        for row in rows:\n",
    "            row['model_id'] = model_id_counter\n",
    "\n",
    "        lambda_stds.append(float(np.std(lam)))\n",
    "        model_depths.append(len(lam) + 1)\n",
    "        input_sens_vals.append(input_sens[:-1])\n",
    "        lambda_vals.append(lam)\n",
    "        data_pts.extend(rows)\n",
    "\n",
    "        raw_props_df = pd.DataFrame(raw_props_dict)\n",
    "        # Define the input shape for profiling (batch size of 1 is sufficient)\n",
    "        profile_input_shape = (1, in_channels, input_size, input_size)\n",
    "        natural_features = extract_natural_features(model, raw_props_df, profile_input_shape)\n",
    "        all_natural_features.append(natural_features)\n",
    "        \n",
    "        model_id_counter += 1\n",
    "        del model\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if not data_pts:\n",
    "        print(\"No valid data collected for this run.\")\n",
    "        return {\n",
    "            'run_id': cfg['run_id'],\n",
    "            'architecture': cfg['architecture'],\n",
    "            'dataset': cfg['dataset']\n",
    "        }\n",
    "\n",
    "    n_models_valid = len(lambda_stds)\n",
    "    print(f\"Generated models accepted       : {n_models_valid}\")\n",
    "    print(f\"Unique datapoints               : {len(set(tuple(d.items()) for d in data_pts))}\")\n",
    "\n",
    "    df          = pd.DataFrame(data_pts)\n",
    "    raw_props   = {c: df[c].values for c in df.columns if c != 'empirical' and c != 'model_id'}\n",
    "    empirical   = df['empirical'].values\n",
    "    input_sens_flat = np.concatenate([f for f in input_sens_vals if f])\n",
    "    lambda_flat = np.concatenate([l for l in lambda_vals if l.size > 0])\n",
    "\n",
    "    complexity_scores = compute_global_complexity(raw_props)\n",
    "    percentile = cfg.get(\"complexity_percentile\", 10)\n",
    "    complexity_labels = classify_models(complexity_scores, percentile)\n",
    "    input_sens_means = np.array([np.mean(f) for f in input_sens_vals if f])\n",
    "    valid_bins = bin_and_validate(input_sens_means, complexity_labels)\n",
    "    per_bin_metrics = compute_bin_metrics(valid_bins, lambda_vals, complexity_scores, raw_props, complexity_labels)\n",
    "    \n",
    "    ratios = []\n",
    "    pvals  = []\n",
    "    for b in valid_bins:\n",
    "       lam_s = [ np.std(lambda_vals[i]) for i in b['indices'] if complexity_labels[i]=='simple' ]\n",
    "       lam_c = [ np.std(lambda_vals[i]) for i in b['indices'] if complexity_labels[i]=='complex' ]\n",
    "       if lam_s and lam_c:\n",
    "           _, p = ks_2samp(lam_s, lam_c)\n",
    "           ratios.append(np.nanmean(lam_s) / np.nanmean(lam_c))\n",
    "           pvals.append(p)\n",
    "    mean_ratio = np.nanmean(ratios) if ratios else np.nan\n",
    "    median_p   = np.nanmedian(pvals) if pvals else np.nan\n",
    "\n",
    "    slope_input_sens = r_value_input_sens = np.nan\n",
    "    grp = None\n",
    "    try:\n",
    "        if 'd' in raw_props and len(raw_props['d']) == len(input_sens_flat):\n",
    "            grp_df = pd.DataFrame({'depth': raw_props['d'], 'input_sens': input_sens_flat})\n",
    "            grp = grp_df.groupby('depth')['input_sens'].agg(['mean','std']).reset_index()\n",
    "            log_d2 = np.log(grp['depth'] + 1e-8)\n",
    "            log_f2 = np.log(grp['mean']  + 1e-8)\n",
    "            slope_input_sens, _, r_value_input_sens, _, _ = linregress(log_d2, log_f2)\n",
    "    except Exception:\n",
    "        grp = None\n",
    "    \n",
    "    print(\"\\n--- Supervised RG-Flow Stability Analysis ---\")\n",
    "    feature_importances, sigma_r2 = pd.DataFrame(), np.nan\n",
    "    if n_models_valid > 10:\n",
    "        X_eng = pd.DataFrame(all_natural_features).fillna(0)\n",
    "        X_eng['mean_IS'] = input_sens_means\n",
    "        y_sigma = np.array(lambda_stds)\n",
    "        \n",
    "        feature_importances, sigma_r2 = train_and_analyze_stability_model(X_eng, y_sigma)\n",
    "        \n",
    "        if not feature_importances.empty:\n",
    "            print(f\"Stability vs Architecture Model R^2 (sigma): {sigma_r2:.3f}\")\n",
    "            print(\"Most Important Architectural Features for Predicting RG-Flow Stability (sigma):\")\n",
    "            print(feature_importances.to_string(index=False))\n",
    "        else:\n",
    "            print(\"Not enough models or data to run supervised analysis.\")\n",
    "    else:\n",
    "        print(\"Skipping supervised analysis due to insufficient models.\")\n",
    "\n",
    "    print(\"\\nIS Scaling with Depth\")\n",
    "    if not np.isnan(slope_input_sens):\n",
    "        print(f\"  IS   ~ depth^{slope_input_sens:.3f}   (R² = {r_value_input_sens**2:.3f})\")\n",
    "        \n",
    "    if valid_bins:\n",
    "        bin_centers = [np.mean([input_sens_means[i] for i in b['indices']]) for b in valid_bins]\n",
    "        all_complexities = [complexity_scores[i] for i in b['indices']]\n",
    "        all_sigmas      = [np.std(lambda_vals[i]) for i in b['indices']]\n",
    "        print(\"\\nBin Analysis Summary\")\n",
    "        print(f\"  Number of valid IS-bins   : {len(valid_bins)}\")\n",
    "        if bin_centers:\n",
    "            print(f\"  IS-bins range             : [{min(bin_centers):.3f}, {max(bin_centers):.3f}]\")\n",
    "        if all_complexities:\n",
    "            print(f\"  Complexity score range    : [{min(all_complexities):.3f}, {max(all_complexities):.3f}]\")\n",
    "        if all_sigmas:\n",
    "            print(f\"  σ range                   : [{min(all_sigmas):.3f}, {max(all_sigmas):.3f}]\")\n",
    "    else:\n",
    "        print(\"\\nBin Analysis Summary\")\n",
    "        print(\"  No valid IS bins — cannot compute summary ranges.\")\n",
    "\n",
    "    print(\"\\nPer-Bin Table (one row per valid bin)\")\n",
    "    print(\"|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\")\n",
    "    print(\"|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\")\n",
    "    if not per_bin_metrics.empty:\n",
    "        bin_centers_lookup = {b['bin_id']: np.mean([input_sens_means[i] for i in b['indices']]) for b in valid_bins}\n",
    "        for _, row in per_bin_metrics.iterrows():\n",
    "            center = bin_centers_lookup.get(row['IS_bin'], 0)\n",
    "            print(f\"| {center:6.2f} | {int(row['#simple']):6d} | {int(row['#complex']):7d} | {row['CV']:7.3f} | {row['ratio']:7.3f} | {row['pval_ratio']:10.3f} | {row['MI_ACS']:8.3f} | {row['MI_pval_ACS']:8.3f} | {row['MI_all_params']:9.3f} | {row['MI_pval_all_params']:11.3f} |\")\n",
    "\n",
    "    print(\"\\nRun-Level Metrics Table\")\n",
    "    if per_bin_metrics.empty:\n",
    "        mean_cv = max_cv = mean_mi_acs = median_mi_acs = max_mi_acs = np.nan\n",
    "        mean_mi_params = median_mi_params = max_mi_params = np.nan\n",
    "    else:\n",
    "        mean_cv = per_bin_metrics['CV'].mean()\n",
    "        max_cv = per_bin_metrics['CV'].max()\n",
    "        mean_mi_acs = per_bin_metrics['MI_ACS'].mean()\n",
    "        median_mi_acs = per_bin_metrics['MI_ACS'].median()\n",
    "        max_mi_acs = per_bin_metrics['MI_ACS'].max()\n",
    "        mean_mi_params = per_bin_metrics['MI_all_params'].mean()\n",
    "        median_mi_params = per_bin_metrics['MI_all_params'].median()\n",
    "        max_mi_params = per_bin_metrics['MI_all_params'].max()\n",
    "\n",
    "    std_ratio = np.nanstd(ratios) if ratios else np.nan\n",
    "    header_fmt = \"| {0:35s} | {1:43s} |\"\n",
    "    row_fmt    = \"| {0:35s} | {1:43s} |\"\n",
    "    print(header_fmt.format(\"Metric\", \"Value\"))\n",
    "    print(\"|\" + \"-\"*37 + \"|\" + \"-\"*45 + \"|\")\n",
    "    print(row_fmt.format(\"Mean CV(bin) ± SD\", f\"{mean_cv:.3f} ± {per_bin_metrics['CV'].std():.3f}\" if not np.isnan(mean_cv) else \"n/a\"))\n",
    "    print(row_fmt.format(\"Max CV(bin)\", f\"{max_cv:.3f}\" if not np.isnan(max_cv) else \"n/a\"))\n",
    "    print(row_fmt.format(\"Ratio σ(simple/complex) ± SD\", f\"{mean_ratio:.3f} ± {std_ratio:.3f}\" if not np.isnan(mean_ratio) else \"n/a\"))\n",
    "    print(row_fmt.format(\"KS p-value σ(simple/complex)\", f\"{median_p:.3f}\" if not np.isnan(median_p) else \"n/a\"))\n",
    "    print(row_fmt.format(\"MI ACS mean ± SD\", f\"{mean_mi_acs:.3f} ± {per_bin_metrics['MI_ACS'].std():.3f}\" if not np.isnan(mean_mi_acs) else \"n/a\"))\n",
    "    print(row_fmt.format(\"MI all_params mean ± SD\", f\"{mean_mi_params:.3f} ± {per_bin_metrics['MI_all_params'].std():.3f}\" if not np.isnan(mean_mi_params) else \"n/a\"))\n",
    "        \n",
    "    ratios_stability = (np.array(lambda_stds)**2) * (np.array(model_depths) - 1)\n",
    "    mean_sigma_stability, std_sigma_stability = np.mean(ratios_stability), np.std(ratios_stability)\n",
    "    cv_sigma_stability = std_sigma_stability / (mean_sigma_stability + 1e-12)\n",
    "    \n",
    "    print(\"\\nRun-level Geometric Stability\")\n",
    "    print(f\"  Mean σ²×(L-1): {mean_sigma_stability:.3e}\")\n",
    "    print(f\"  Std  σ²×(L-1): {std_sigma_stability:.3e}\")\n",
    "    print(f\"  CV   σ²×(L-1): {cv_sigma_stability:.3f}\")\n",
    "    \n",
    "    if cfg.get('make_plots', False) and (grp is not None):\n",
    "        inv_Lminus1 = 1.0 / (np.array(model_depths) - 1)\n",
    "        sigma2      = np.array(lambda_stds)**2\n",
    "        plot_key_results({'grouped': grp, 'inv_Lminus1': inv_Lminus1, 'sigma2': sigma2}, title_suffix = f\" ({cfg['run_id']})\")\n",
    "\n",
    "    final_results = {\n",
    "        'run_id': cfg['run_id'],\n",
    "        'architecture': cfg['architecture'],\n",
    "        'dataset': cfg['dataset'],\n",
    "        'n_models_accepted': n_models_valid,\n",
    "        'n_models_used_in_bins': sum(len(b['indices']) for b in valid_bins),\n",
    "        'n_lambda_vals': len(lambda_flat),\n",
    "        'n_input_sens_vals': len(input_sens_flat),\n",
    "        'n_IS_bins_attempted': cfg.get('n_variance_levels', 20),\n",
    "        'n_IS_bins_valid': len(valid_bins),\n",
    "        'input_sens_depth_slope': slope_input_sens,\n",
    "        'input_sens_depth_r2': r_value_input_sens**2 if not np.isnan(r_value_input_sens) else np.nan,\n",
    "        'mean_sigma_ratio_simple_to_complex': mean_ratio,\n",
    "        'ks_pvalue_sigma_ratio': median_p,\n",
    "        'cv_bins_mean': mean_cv,\n",
    "        'cv_bins_std':  np.nan if per_bin_metrics.empty else per_bin_metrics['CV'].std(),\n",
    "        'cv_bins_max':  max_cv,\n",
    "        'mean_mi_acs': mean_mi_acs,\n",
    "        'median_mi_acs': median_mi_acs,\n",
    "        'max_mi_acs': max_mi_acs,\n",
    "        'mean_mi_all_params': mean_mi_params,\n",
    "        'median_mi_all_params': median_mi_params,\n",
    "        'max_mi_all_params': max_mi_params,\n",
    "        'mean_sigma2_ratio': mean_sigma_stability,\n",
    "        'std_sigma2_ratio': std_sigma_stability,\n",
    "        'cv_sigma2_ratio': cv_sigma_stability,\n",
    "        'rg_flow_model_r2': sigma_r2,\n",
    "        'rg_flow_feature_importances': feature_importances.to_dict('records') if not feature_importances.empty else [],\n",
    "    }\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def analyze_experiment_results(results_list):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Helper to compute mean ± 95% CI\n",
    "    def mean_ci(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) < 2:\n",
    "            return None\n",
    "        m = a.mean()\n",
    "        se = stats.sem(a)\n",
    "        h = stats.t.ppf(0.975, len(a)-1) * se\n",
    "        return m, m-h, m+h\n",
    "\n",
    "    # Format an array as mean ± half-width (95% CI, n=…)\n",
    "    def fmt_ci(arr):\n",
    "        ci = mean_ci(arr)\n",
    "        cnt = np.count_nonzero(~np.isnan(arr))\n",
    "        if ci is None:\n",
    "            return \"n/a\"\n",
    "        m, lo, hi = ci\n",
    "        return f\"{m:.3f} ± {(m-lo):.3f} (95% CI, n={cnt})\"\n",
    "\n",
    "    # Format p-values as median [IQR] (n=…)\n",
    "    def fmt_p(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) == 0:\n",
    "            return \"n/a\"\n",
    "        q25, q75 = np.percentile(a, [25, 75])\n",
    "        return f\"{np.median(a):.3f} [IQR {q25:.3f}, {q75:.3f}] (n={len(a)})\"\n",
    "\n",
    "    # Simple mean±SD\n",
    "    def fmt_sd(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) == 0:\n",
    "            return \"n/a\"\n",
    "        return f\"{a.mean():.3f} ± {a.std():.3f}\"\n",
    "\n",
    "    n_runs = len(results_list)\n",
    "\n",
    "    # collect arrays across all runs, using .get to default to nan\n",
    "    ratio_arr   = [r.get('mean_sigma_ratio_simple_to_complex',   np.nan) for r in results_list]\n",
    "    pval_arr    = [r.get('ks_pvalue_sigma_ratio',               np.nan) for r in results_list]\n",
    "    cv_arr      = [r.get('cv_bins_mean',                        np.nan) for r in results_list]\n",
    "    cvmax_arr   = [r.get('cv_bins_max',                         np.nan) for r in results_list]\n",
    "    mi_acs_arr  = [r.get('mean_mi_acs',                         np.nan) for r in results_list]\n",
    "    mi_ap_arr   = [r.get('mean_mi_all_params',                  np.nan) for r in results_list]\n",
    "    s2_m_arr    = [r.get('mean_sigma2_ratio',                   np.nan) for r in results_list]\n",
    "    s2_s_arr    = [r.get('std_sigma2_ratio',                    np.nan) for r in results_list]\n",
    "    s2_cv_arr   = [r.get('cv_sigma2_ratio',                     np.nan) for r in results_list]\n",
    "\n",
    "    # --- CROSS-RUN SUMMARY ---\n",
    "    print(\"\\n===== CROSS-RUN SUMMARY =====\\n\")\n",
    "    # Metadata\n",
    "    tm = sum(r.get('n_models_accepted',0) for r in results_list)\n",
    "    tl = sum(r.get('n_lambda_vals',0)       for r in results_list)\n",
    "    tf = sum(r.get('n_input_sens_vals',0)       for r in results_list)\n",
    "    ba = sum(r.get('n_IS_bins_attempted',0) for r in results_list)\n",
    "    bv = sum(r.get('n_IS_bins_valid',0)     for r in results_list)\n",
    "    ub = np.nanmean([r.get('n_models_used_in_bins', np.nan) for r in results_list])\n",
    "    print(\"Metadata:\")\n",
    "    print(f\"  Total models accepted          : {tm}\")\n",
    "    print(f\"  Total λᵢ values                : {tl}\")\n",
    "    print(f\"  Total IS values            : {tf}\")\n",
    "    print(f\"  Valid IS bins                  : {bv}\")\n",
    "    print(f\"  Total IS bins attempted        : {ba}\")\n",
    "    print(f\"  Avg models per run             : {tm/n_runs:.2f}\")\n",
    "    print(f\"  Avg λᵢ per model               : {tl/tm:.2f}\")\n",
    "    print(f\"  Avg models used-in-bins per run: {ub:.2f}\\n\")\n",
    "\n",
    "    # 1. Per-IS-bin σ Variation\n",
    "    print(\"1. Per-IS-bin σ Variation (Old Method):\")\n",
    "    print(f\"   Ratio σ(simple/complex)  : {fmt_ci(ratio_arr)}\")\n",
    "    print(f\"   KS-p-value summary       : {fmt_p(pval_arr)}\")\n",
    "    print(f\"   Runs with p<0.05         : {int(np.sum(np.array(pval_arr)<0.05))} / {n_runs}\\n\")\n",
    "\n",
    "    # 2. Per-IS-bin CV(σ)\n",
    "    print(\"2. Per-IS-bin CV(σ):\")\n",
    "    print(f\"   Mean CV(bin) : {fmt_ci(cv_arr)}\")\n",
    "    print(f\"   Max  CV(bin) : {fmt_ci(cvmax_arr)}\\n\")\n",
    "\n",
    "    # 3. Mutual Information ACS\n",
    "    print(\"3. Mutual Information ACS (Old Method):\")\n",
    "    if all(np.isnan(mi_acs_arr)):\n",
    "        print(\"   MI_ACS: n/a\\n\")\n",
    "    else:\n",
    "        print(f\"   Mean ± SD     : {fmt_sd(mi_acs_arr)}\")\n",
    "        med = np.nanmedian(mi_acs_arr)\n",
    "        iqr = (np.nanpercentile(mi_acs_arr,25), np.nanpercentile(mi_acs_arr,75))\n",
    "        print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "        print(f\"   Max           : {np.nanmax(mi_acs_arr):.3f}\\n\")\n",
    "\n",
    "    # 4. Mutual Information all_params\n",
    "    print(\"4. Mutual Information all_params (Old Method):\")\n",
    "    if all(np.isnan(mi_ap_arr)):\n",
    "        print(\"   MI_all_params: n/a\\n\")\n",
    "    else:\n",
    "        print(f\"   Mean ± SD     : {fmt_sd(mi_ap_arr)}\")\n",
    "        med = np.nanmedian(mi_ap_arr)\n",
    "        iqr = (np.nanpercentile(mi_ap_arr,25), np.nanpercentile(mi_ap_arr,75))\n",
    "        print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "        print(f\"   Max           : {np.nanmax(mi_ap_arr):.3f}\\n\")\n",
    "\n",
    "    # 5. Stability σ²×(L-1)\n",
    "    print(\"5. Stability σ²×(L-1):\")\n",
    "    print(f\"   Mean : {fmt_ci(s2_m_arr)}\")\n",
    "    print(f\"   Std  : {fmt_ci(s2_s_arr)}\")\n",
    "    print(f\"   CV   : {fmt_ci(s2_cv_arr)}\\n\")\n",
    "\n",
    "    # --- NEW SECTION 6: RG-FLOW ANALYSIS ---\n",
    "    print(\"6. RG-Flow Stability Prediction (New Supervised Method):\")\n",
    "    r2_arr = [r.get('rg_flow_model_r2', np.nan) for r in results_list]\n",
    "    print(f\"   Mean Model R^2 (across runs) : {fmt_ci(r2_arr)}\")\n",
    "\n",
    "    agg_importances = defaultdict(list)\n",
    "    for r in results_list:\n",
    "        if 'rg_flow_feature_importances' in r and r['rg_flow_feature_importances']:\n",
    "            for item in r['rg_flow_feature_importances']:\n",
    "                agg_importances[item['feature']].append(item['importance'])\n",
    "    \n",
    "    if agg_importances:\n",
    "        mean_imps = {name: np.mean(vals) for name, vals in agg_importances.items()}\n",
    "        sorted_imps = sorted(mean_imps.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        print(\"\\n   Aggregated Feature Importances (Top 5):\")\n",
    "        print(\"   \" + \"-\"*45)\n",
    "        print(f\"   {'Feature':<30} | {'Mean Importance':<15}\")\n",
    "        print(\"   \" + \"-\"*45)\n",
    "        for feature, importance in sorted_imps[:5]:\n",
    "            print(f\"   {feature:<30} | {importance:<15.4f}\")\n",
    "        print(\"   \" + \"-\"*45 + \"\\n\")\n",
    "    else:\n",
    "        print(\"   No feature importance data to report.\\n\")\n",
    "\n",
    "\n",
    "    # --- PER-ARCH/DATASET BREAKDOWN ---\n",
    "    # (This section is unchanged, but will now implicitly benefit from the new data)\n",
    "    print(\"\\n===== PER-ARCH/DATASET BREAKDOWN =====\\n\")\n",
    "    by_group = defaultdict(list)\n",
    "    for r in results_list:\n",
    "        by_group[(r.get('architecture','?'), r.get('dataset','?'))].append(r)\n",
    "\n",
    "    for (arch, ds), grp in by_group.items():\n",
    "        print(f\"--- {arch.upper()} / {ds.upper()} (n={len(grp)}) ---\")\n",
    "        # You can add a mini version of the RG-Flow analysis here if desired,\n",
    "        # but for simplicity, we'll leave this section as is. The overall\n",
    "        # summary in section 6 is the most important part.\n",
    "        g_ratio = [g.get('mean_sigma_ratio_simple_to_complex', np.nan) for g in grp]\n",
    "        g_pval  = [g.get('ks_pvalue_sigma_ratio',             np.nan) for g in grp]\n",
    "        g_cv    = [g.get('cv_bins_mean',                       np.nan) for g in grp]\n",
    "        g_cvmax = [g.get('cv_bins_max',                        np.nan) for g in grp]\n",
    "        g_s2_m  = [g.get('mean_sigma2_ratio',                  np.nan) for g in grp]\n",
    "        g_s2_s  = [g.get('std_sigma2_ratio',                   np.nan) for g in grp]\n",
    "        g_s2_cv = [g.get('cv_sigma2_ratio',                    np.nan) for g in grp]\n",
    "        g_r2    = [g.get('rg_flow_model_r2',                   np.nan) for g in grp]\n",
    "\n",
    "        print(\"1. Per-IS-bin σ Variation:\")\n",
    "        print(f\"   Ratio σ(simple/complex)  : {fmt_ci(g_ratio)}\")\n",
    "        print(f\"   KS-p-value summary       : {fmt_p(g_pval)}\\n\")\n",
    "        \n",
    "        print(\"2. Per-IS-bin CV(σ):\")\n",
    "        print(f\"   Mean CV(bin) : {fmt_ci(g_cv)}\")\n",
    "        print(f\"   Max  CV(bin) : {fmt_ci(g_cvmax)}\\n\")\n",
    "\n",
    "        print(\"3. Stability σ²×(L-1):\")\n",
    "        print(f\"   Mean : {fmt_ci(g_s2_m)}\")\n",
    "        print(f\"   Std  : {fmt_ci(g_s2_s)}\")\n",
    "        print(f\"   CV   : {fmt_ci(g_s2_cv)}\\n\")\n",
    "        \n",
    "        print(\"4. RG-Flow Stability Prediction (R^2):\")\n",
    "        print(f\"   Mean Model R^2 : {fmt_ci(g_r2)}\\n\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"overall\": {\n",
    "            \"ratio_CI\": fmt_ci(ratio_arr),\n",
    "            \"ks_p\":     fmt_p(pval_arr),\n",
    "            \"cv_mean\":  fmt_ci(cv_arr),\n",
    "            \"cv_max\":   fmt_ci(cvmax_arr),\n",
    "            \"mi_acs\":   fmt_sd(mi_acs_arr) if not all(np.isnan(mi_acs_arr)) else \"n/a\",\n",
    "            \"mi_ap\":    fmt_sd(mi_ap_arr)  if not all(np.isnan(mi_ap_arr))  else \"n/a\",\n",
    "            \"s2_mean\":  fmt_ci(s2_m_arr),\n",
    "            \"s2_std\":   fmt_ci(s2_s_arr),\n",
    "            \"s2_cv\":    fmt_ci(s2_cv_arr),\n",
    "            \"rg_flow_r2\": fmt_ci(r2_arr),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_statistical_consistency_experiments(\n",
    "    seeds,\n",
    "    architectures,\n",
    "    datasets,\n",
    "    data_fraction,\n",
    "    # The 'noise_levels' parameter is no longer used and can be removed\n",
    "    n_models=100,\n",
    "    complexity_percentile=10,\n",
    "    n_variance_levels=20,\n",
    "    n_bootstrap=50,\n",
    "    make_plots=False,\n",
    "    architecture_configs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs experiments over all combinations of seed, architecture, and dataset.\n",
    "    The notion of a single noise_level is removed in favor of a robust lambda calculation.\n",
    "    \"\"\"\n",
    "    # Remove `noise_levels` from the total calculation and the inner loop\n",
    "    total = len(seeds) * len(architectures) * len(datasets)\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        for arch in architectures:\n",
    "            for ds in datasets:\n",
    "                count += 1\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(f\"Experiment {count}/{total}: \"\n",
    "                      f\"arch={arch}, dataset={ds}, seed={seed}\")\n",
    "\n",
    "                # The config no longer needs a specific `noise_std`\n",
    "                cfg = {\n",
    "                    'seed': seed,\n",
    "                    'architecture': arch,\n",
    "                    'dataset': ds,\n",
    "                    'data_fraction': data_fraction,\n",
    "                    'n_models': n_models,\n",
    "                    'complexity_percentile': complexity_percentile,\n",
    "                    'n_variance_levels': n_variance_levels,\n",
    "                    'n_bootstrap': n_bootstrap,\n",
    "                    'make_plots': make_plots,\n",
    "                    'run_id': f\"{arch}_{ds}_s{seed}\",\n",
    "                    **architecture_configs[arch]\n",
    "                }\n",
    "\n",
    "                res = run_experiment(cfg)\n",
    "                all_results.append(res)\n",
    "\n",
    "    analysis = analyze_experiment_results(all_results)\n",
    "    return all_results, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6370208b-6a58-4231-9db0-61ed4c05d926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:54:31.192351Z",
     "iopub.status.busy": "2025-06-30T16:54:31.191804Z",
     "iopub.status.idle": "2025-06-30T17:00:42.077135Z",
     "shell.execute_reply": "2025-06-30T17:00:42.076538Z",
     "shell.execute_reply.started": "2025-06-30T16:54:31.192328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment 1/8: arch=cnn, dataset=cifar, seed=1\n",
      "Running run_id       : cnn_cifar_s1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Files already downloaded and verified\n",
      "Input shape          : (4, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 30/30 [01:56<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 30\n",
      "Unique datapoints               : 1074\n",
      "\n",
      "--- Supervised RG-Flow Stability Analysis ---\n",
      "Stability vs Architecture Model R^2 (sigma): 0.901\n",
      "Most Important Architectural Features for Predicting RG-Flow Stability (sigma):\n",
      "               feature  importance\n",
      "      layers_per_stage    0.553929\n",
      "           mean_stride    0.416045\n",
      "          mean_overlap    0.008549\n",
      "    mean_channel_ratio    0.006241\n",
      "      initial_channels    0.004428\n",
      "               mean_IS    0.004309\n",
      "         param_density    0.002003\n",
      "          max_channels    0.001375\n",
      "        total_macs_log    0.001192\n",
      "width_expansion_factor    0.001025\n",
      "     std_channel_ratio    0.000888\n",
      "   downsampling_stages    0.000016\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-3.884   (R² = 0.580)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [36.272, 36.272]\n",
      "  Complexity score range    : [-0.833, 0.795]\n",
      "  σ range                   : [1.207, 8.494]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  36.27 |      9 |       3 |   0.436 |   0.715 |      0.455 |    0.090 |    0.234 |     0.000 |       1.000 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.436 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.436                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 0.715 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.455                                       |\n",
      "| MI ACS mean ± SD                    | 0.090 ± nan                                 |\n",
      "| MI all_params mean ± SD             | 0.000 ± nan                                 |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 1.319e+03\n",
      "  Std  σ²×(L-1): 1.298e+03\n",
      "  CV   σ²×(L-1): 0.984\n",
      "\n",
      "============================================================\n",
      "Experiment 2/8: arch=cnn, dataset=mnist, seed=1\n",
      "Running run_id       : cnn_mnist_s1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 39\n",
      "Input shape          : (4, 1, 39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 30/30 [01:45<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 30\n",
      "Unique datapoints               : 1074\n",
      "\n",
      "--- Supervised RG-Flow Stability Analysis ---\n",
      "Stability vs Architecture Model R^2 (sigma): 0.670\n",
      "Most Important Architectural Features for Predicting RG-Flow Stability (sigma):\n",
      "               feature  importance\n",
      "      layers_per_stage    0.767296\n",
      "      initial_channels    0.073599\n",
      "           mean_stride    0.059844\n",
      "width_expansion_factor    0.026406\n",
      "    mean_channel_ratio    0.021729\n",
      "        total_macs_log    0.020855\n",
      "          mean_overlap    0.020091\n",
      "               mean_IS    0.004049\n",
      "     std_channel_ratio    0.002830\n",
      "         param_density    0.002509\n",
      "          max_channels    0.000793\n",
      "   downsampling_stages    0.000000\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-3.868   (R² = 0.571)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [33.684, 33.684]\n",
      "  Complexity score range    : [-0.833, 0.795]\n",
      "  σ range                   : [1.373, 8.286]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  33.68 |      9 |       3 |   0.415 |   0.794 |      0.709 |    0.000 |    1.000 |     0.000 |       1.000 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.415 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.415                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 0.794 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.709                                       |\n",
      "| MI ACS mean ± SD                    | 0.000 ± nan                                 |\n",
      "| MI all_params mean ± SD             | 0.000 ± nan                                 |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 1.342e+03\n",
      "  Std  σ²×(L-1): 1.437e+03\n",
      "  CV   σ²×(L-1): 1.070\n",
      "\n",
      "============================================================\n",
      "Experiment 3/8: arch=resnet, dataset=cifar, seed=1\n",
      "Running run_id       : resnet_cifar_s1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (4, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models:  43%|████▎     | 13/30 [02:23<03:08, 11.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m      2\u001b[0m architecture_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth_choices\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     }\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# call to main script:\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m results, analysis \u001b[38;5;241m=\u001b[39m \u001b[43mrun_statistical_consistency_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43marchitectures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# proportion of test set to sample\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# how many random nets per run\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplexity_percentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# top/bottom 25% are “complex”/“simple”\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_variance_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# how many IS‐bins to attempt\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmake_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# toggle your two matplotlib plots\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43marchitecture_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marchitecture_configs\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis)\n",
      "Cell \u001b[0;32mIn[3], line 1290\u001b[0m, in \u001b[0;36mrun_statistical_consistency_experiments\u001b[0;34m(seeds, architectures, datasets, data_fraction, n_models, complexity_percentile, n_variance_levels, n_bootstrap, make_plots, architecture_configs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[38;5;66;03m# The config no longer needs a specific `noise_std`\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m             cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1277\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: seed,\n\u001b[1;32m   1278\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: arch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1287\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marchitecture_configs[arch]\n\u001b[1;32m   1288\u001b[0m             }\n\u001b[0;32m-> 1290\u001b[0m             res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m             all_results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m   1293\u001b[0m analysis \u001b[38;5;241m=\u001b[39m analyze_experiment_results(all_results)\n",
      "Cell \u001b[0;32mIn[3], line 857\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    849\u001b[0m     pr \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_ranges\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    850\u001b[0m     model \u001b[38;5;241m=\u001b[39m generate_random_resnet(\n\u001b[1;32m    851\u001b[0m         depth, pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m], input_size, in_channels,\n\u001b[1;32m    852\u001b[0m         pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m], pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m], pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    853\u001b[0m         pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottleneck_ratios\u001b[39m\u001b[38;5;124m'\u001b[39m], pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojection_types\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    854\u001b[0m         pr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_functions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    855\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 857\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_measure_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[3], line 825\u001b[0m, in \u001b[0;36mrun_experiment.<locals>._measure_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 825\u001b[0m     input_sens \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_input_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m props \u001b[38;5;241m=\u001b[39m extract_model_properties(model)\n\u001b[1;32m    827\u001b[0m ml \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lam)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(input_sens))\n",
      "Cell \u001b[0;32mIn[3], line 588\u001b[0m, in \u001b[0;36mcompute_input_sensitivity\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m acts:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# exactly the same mathematical expression as before\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     loss \u001b[38;5;241m=\u001b[39m act\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m--> 588\u001b[0m     (grad,) \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     input_sens\u001b[38;5;241m.\u001b[39mappend(grad\u001b[38;5;241m.\u001b[39mnorm()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_sens\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# architecture_configs\n",
    "architecture_configs = {\n",
    "    'cnn': {\n",
    "        'depth_choices': [4, 6, 8, 10, 20, 40, 80, 100],\n",
    "        'resize_range': (1.0, 4.0),\n",
    "        'param_ranges': {\n",
    "            'strides':  [1, 2, 3],\n",
    "            'kernels':  [3, 5, 7, 9, 11, 13, 15, 17],\n",
    "            'channels': [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "        }\n",
    "    },\n",
    "    'resnet': {\n",
    "        'depth_choices': [4, 6, 8, 10, 20, 40, 80, 100],\n",
    "        'resize_range': (1.0, 4.0),\n",
    "        'param_ranges': {\n",
    "            'channels':          [16, 32, 64, 128, 256, 512, 1024],\n",
    "            'block_sizes':       [1, 2, 3, 4, 5],\n",
    "            'kernel_sizes':      [1, 3, 5, 7, 9],\n",
    "            'strides':           [1, 2, 3],\n",
    "            'bottleneck_ratios': [1, 0.75, 0.5, 0.25, 0.125],\n",
    "            'projection_types':  ['identity', 'conv1x1'],\n",
    "            'activation_functions': ['relu', 'leaky_relu']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# call to main script:\n",
    "results, analysis = run_statistical_consistency_experiments(\n",
    "    seeds=[1, 12],\n",
    "    architectures=['cnn', 'resnet'],\n",
    "    datasets=['cifar', 'mnist'],\n",
    "    data_fraction=0.0004,          # proportion of test set to sample\n",
    "    n_models=30,                  # how many random nets per run\n",
    "    complexity_percentile=25,      # top/bottom 25% are “complex”/“simple”\n",
    "    n_variance_levels=10,          # how many IS‐bins to attempt\n",
    "    n_bootstrap=20,\n",
    "    make_plots=False,               # toggle your two matplotlib plots\n",
    "    architecture_configs=architecture_configs\n",
    ")\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc30076-72d0-4a11-93ca-763b263c1ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T17:12:54.497836Z",
     "iopub.status.busy": "2025-06-30T17:12:54.497592Z",
     "iopub.status.idle": "2025-06-30T17:12:54.518428Z",
     "shell.execute_reply": "2025-06-30T17:12:54.517846Z",
     "shell.execute_reply.started": "2025-06-30T17:12:54.497818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Part 1: Imports and Necessary Building Blocks (Unchanged)\n",
    "# ===================================================================\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from thop import profile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class ModularCNN(nn.Module):\n",
    "    def __init__(self, conv_channels, kernel_sizes, strides, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.strides_list = strides\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for out_ch, k, s in zip(conv_channels, kernel_sizes, strides):\n",
    "            conv = nn.Conv2d(in_channels, out_ch, kernel_size=k, padding=k//2, bias=True)\n",
    "            self.convs.append(conv)\n",
    "            in_channels = out_ch\n",
    "\n",
    "        # Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(conv_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            x = F.relu(x)\n",
    "            s = self.strides_list[idx]\n",
    "            if s > 1:\n",
    "                x = F.avg_pool2d(x, kernel_size=s)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ===================================================================\n",
    "# Part 2: The New, More Powerful Model Engineering Function\n",
    "# ===================================================================\n",
    "\n",
    "### --- NEW/MODIFIED --- ###\n",
    "def generate_controlled_cnn_v2(\n",
    "    design_type: str,\n",
    "    target_value: float,\n",
    "    control_variable: str, # 'macs' or 'params'\n",
    "    input_shape: tuple,\n",
    "    tolerance: float = 0.05, # Tighter tolerance for better matching\n",
    "    max_iter: int = 50\n",
    ") -> ModularCNN:\n",
    "    \"\"\"\n",
    "    Engineers a CNN to have a target budget for either MACs or Parameters.\n",
    "\n",
    "    Args:\n",
    "        design_type (str): 'stable' or 'erratic'.\n",
    "        target_value (float): The desired value for the control variable.\n",
    "        control_variable (str): What to target: 'macs' or 'params'.\n",
    "        input_shape (tuple): The input shape for MACs calculation.\n",
    "        tolerance (float): The allowed relative error.\n",
    "        max_iter (int): Maximum attempts to find a matching model.\n",
    "\n",
    "    Returns:\n",
    "        A ModularCNN instance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Engineering '{design_type.upper()}' model (Controlling for {control_variable.upper()}) ---\")\n",
    "    if control_variable == 'macs':\n",
    "        print(f\"Target MACs: {target_value / 1e6:.1f}M\")\n",
    "    else:\n",
    "        print(f\"Target Params: {target_value / 1e6:.2f}M\")\n",
    "\n",
    "    # Define the core design principle based on our findings\n",
    "    if design_type == 'stable':\n",
    "        strides = [1, 2, 1, 2, 1, 2] # \"wide and fast-shrinking\"\n",
    "        base_channels = 32\n",
    "    elif design_type == 'erratic':\n",
    "        strides = [1, 1, 1, 1, 2, 1, 1, 1, 1, 2] # \"narrow and slow-shrinking\"\n",
    "        base_channels = 16\n",
    "    else:\n",
    "        raise ValueError(\"design_type must be 'stable' or 'erratic'\")\n",
    "    \n",
    "    depth = len(strides)\n",
    "    kernels = [3] * depth\n",
    "\n",
    "    # Iteratively adjust channel width to meet the budget\n",
    "    for i in range(max_iter):\n",
    "        channels = [int(base_channels * (1.5**s)) for s in range(len(strides))]\n",
    "        model = ModularCNN(conv_channels=channels, kernel_sizes=kernels, strides=strides)\n",
    "        \n",
    "        dummy_input = torch.randn(input_shape)\n",
    "        macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "\n",
    "        current_value = macs if control_variable == 'macs' else params\n",
    "        \n",
    "        if abs(current_value - target_value) / target_value < tolerance:\n",
    "            print(f\"Success! Model has {macs/1e6:.1f}M MACs and {params/1e6:.2f}M params.\")\n",
    "            print(f\"Design -> Strides: {strides}, Channels: {channels[:4]}...\")\n",
    "            return model.to(device)\n",
    "\n",
    "        # Adjust for next iteration based on a simple scaling rule\n",
    "        if control_variable == 'macs':\n",
    "             # MACs scale roughly with channels^2\n",
    "            ratio = np.sqrt(target_value / (current_value + 1e-9))\n",
    "        else: # params\n",
    "            # Params also scale roughly with channels^2 in conv layers\n",
    "            ratio = np.sqrt(target_value / (current_value + 1e-9))\n",
    "        base_channels = int(base_channels * ratio)\n",
    "        if base_channels < 1: base_channels = 1\n",
    "        \n",
    "    raise RuntimeError(f\"Failed to generate a model with target {control_variable} after {max_iter} iterations.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Part 3: Training and Evaluation Infrastructure (Unchanged)\n",
    "# ===================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        pbar.set_postfix(loss=running_loss/total, acc=100.*correct/total)\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            pbar.set_postfix(loss=running_loss/total, acc=100.*correct/total)\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# ===================================================================\n",
    "# Part 4: The New \"Three-Point Comparison\" Experiment Runner\n",
    "# ===================================================================\n",
    "\n",
    "### --- NEW/MODIFIED --- ###\n",
    "def run_three_point_comparison(config):\n",
    "    set_random_seed(config['seed'])\n",
    "\n",
    "    # --- 1. Data Loading ---\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    if config['data_fraction'] < 1.0:\n",
    "        # Code to use a subset of data (unchanged)\n",
    "        pass\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    # --- 2. The Three-Point Model Generation ---\n",
    "    input_shape = (1, 3, 32, 32)\n",
    "    \n",
    "    # Model A: The 'Stable' baseline, controlling for a target parameter count.\n",
    "    model_a = generate_controlled_cnn_v2('stable', config['target_params'], 'params', input_shape)\n",
    "    _, model_a_params = profile(model_a, inputs=(torch.randn(input_shape).to(device),), verbose=False)\n",
    "    model_a_macs, _ = profile(model_a, inputs=(torch.randn(input_shape).to(device),), verbose=False)\n",
    "    \n",
    "    # Model B: 'Erratic' model matched to Model A's MACs.\n",
    "    # This replicates our original flawed \"Erratic\" model.\n",
    "    model_b = generate_controlled_cnn_v2('erratic', model_a_macs, 'macs', input_shape)\n",
    "\n",
    "    # Model C: 'Erratic' model matched to Model A's Parameters.\n",
    "    # This is the crucial new control model.\n",
    "    model_c = generate_controlled_cnn_v2('erratic', model_a_params, 'params', input_shape)\n",
    "    \n",
    "    # --- 3. Training Setup ---\n",
    "    models = {\n",
    "        f\"A: Stable (Low σ)\": model_a,\n",
    "        f\"B: Erratic (MACs-Matched)\": model_b,\n",
    "        f\"C: Erratic (Params-Matched)\": model_c,\n",
    "    }\n",
    "    optimizers = {name: optim.Adam(model.parameters(), lr=config['lr']) for name, model in models.items()}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    results = {name: {'val_acc': []} for name in models.keys()}\n",
    "\n",
    "    # --- 4. The Race ---\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        for name, model in models.items():\n",
    "            print(f\"--- Training {name} Model ---\")\n",
    "            train_one_epoch(model, train_loader, criterion, optimizers[name], device)\n",
    "            _, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            results[name]['val_acc'].append(val_acc)\n",
    "            print(f\"Epoch {epoch+1} [{name}] - Val Acc: {val_acc:.2f}%\")\n",
    "    return results\n",
    "\n",
    "# ===================================================================\n",
    "# Part 5: Modified Visualization\n",
    "# ===================================================================\n",
    "\n",
    "### --- NEW/MODIFIED --- ###\n",
    "def plot_three_point_results(results, config):\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#d62728'] # Blue, Orange, Red\n",
    "    for i, (name, metrics) in enumerate(results.items()):\n",
    "        ax.plot(metrics['val_acc'], label=f'{name}', color=colors[i], marker='o', markersize=4, linestyle='--')\n",
    "\n",
    "    ax.set_title(f\"3-Point Comparison: The Impact of Architectural Shape vs. Capacity\\n(Target Params: {config['target_params']/1e6:.2f}M)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    ax.set_ylabel(\"Validation Accuracy (%)\", fontsize=12)\n",
    "    ax.legend(fontsize=12, title=\"Models\")\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a48760-8984-469c-8371-05c0bb6dc861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T17:12:59.852455Z",
     "iopub.status.busy": "2025-06-30T17:12:59.851886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Engineering 'STABLE' model (Controlling for PARAMS) ---\n",
      "Target Params: 0.60M\n",
      "Success! Model has 68.3M MACs and 0.59M params.\n",
      "Design -> Strides: [1, 2, 1, 2, 1, 2], Channels: [31, 46, 69, 104]...\n",
      "\n",
      "--- Engineering 'ERRATIC' model (Controlling for MACS) ---\n",
      "Target MACs: 68.3M\n",
      "Success! Model has 67.8M MACs and 0.25M params.\n",
      "Design -> Strides: [1, 1, 1, 1, 2, 1, 1, 1, 1, 2], Channels: [4, 6, 9, 13]...\n",
      "\n",
      "--- Engineering 'ERRATIC' model (Controlling for PARAMS) ---\n",
      "Target Params: 0.59M\n",
      "Success! Model has 152.9M MACs and 0.57M params.\n",
      "Design -> Strides: [1, 1, 1, 1, 2, 1, 1, 1, 1, 2], Channels: [6, 9, 13, 20]...\n",
      "\n",
      "Epoch 1/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [A: Stable (Low σ)] - Val Acc: 45.07%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [B: Erratic (MACs-Matched)] - Val Acc: 34.15%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [C: Erratic (Params-Matched)] - Val Acc: 34.49%\n",
      "\n",
      "Epoch 2/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [A: Stable (Low σ)] - Val Acc: 54.19%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [B: Erratic (MACs-Matched)] - Val Acc: 42.67%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [C: Erratic (Params-Matched)] - Val Acc: 45.15%\n",
      "\n",
      "Epoch 3/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [A: Stable (Low σ)] - Val Acc: 60.34%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [B: Erratic (MACs-Matched)] - Val Acc: 47.08%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [C: Erratic (Params-Matched)] - Val Acc: 50.98%\n",
      "\n",
      "Epoch 4/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [A: Stable (Low σ)] - Val Acc: 64.33%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [B: Erratic (MACs-Matched)] - Val Acc: 49.94%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [C: Erratic (Params-Matched)] - Val Acc: 55.31%\n",
      "\n",
      "Epoch 5/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [A: Stable (Low σ)] - Val Acc: 66.61%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [B: Erratic (MACs-Matched)] - Val Acc: 51.90%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [C: Erratic (Params-Matched)] - Val Acc: 56.54%\n",
      "\n",
      "Epoch 6/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [A: Stable (Low σ)] - Val Acc: 70.57%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [B: Erratic (MACs-Matched)] - Val Acc: 57.74%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [C: Erratic (Params-Matched)] - Val Acc: 59.39%\n",
      "\n",
      "Epoch 7/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [A: Stable (Low σ)] - Val Acc: 70.91%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [B: Erratic (MACs-Matched)] - Val Acc: 59.33%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [C: Erratic (Params-Matched)] - Val Acc: 62.06%\n",
      "\n",
      "Epoch 8/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [A: Stable (Low σ)] - Val Acc: 72.05%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [B: Erratic (MACs-Matched)] - Val Acc: 58.39%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [C: Erratic (Params-Matched)] - Val Acc: 63.79%\n",
      "\n",
      "Epoch 9/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [A: Stable (Low σ)] - Val Acc: 74.60%\n",
      "--- Training B: Erratic (MACs-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [B: Erratic (MACs-Matched)] - Val Acc: 63.92%\n",
      "--- Training C: Erratic (Params-Matched) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [C: Erratic (Params-Matched)] - Val Acc: 65.04%\n",
      "\n",
      "Epoch 10/10\n",
      "--- Training A: Stable (Low σ) Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 90/391 [00:01<00:03, 92.78it/s, acc=78.3, loss=0.00481]"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# Part 6: Main Execution Block\n",
    "# ===================================================================\n",
    "\n",
    "experiment_config = {\n",
    "    'seed': 42,\n",
    "    'target_params': 600000, # Target 0.6 Million Parameters\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-3,\n",
    "    'batch_size': 128,\n",
    "    'data_fraction': 1.0,\n",
    "}\n",
    "\n",
    "training_results = run_three_point_comparison(experiment_config)\n",
    "plot_three_point_results(training_results, experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ca0c08-85b3-4063-8acb-824f182cc649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T08:06:44.162123Z",
     "iopub.status.busy": "2025-07-01T08:06:44.161889Z",
     "iopub.status.idle": "2025-07-01T08:06:44.193321Z",
     "shell.execute_reply": "2025-07-01T08:06:44.192844Z",
     "shell.execute_reply.started": "2025-07-01T08:06:44.162106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Part 1: Imports and Core Building Blocks\n",
    "# ===================================================================\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from thop import profile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "### --- RESNET & CNN DEFINITIONS --- ###\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ModularResNet(nn.Module):\n",
    "    def __init__(self, block_config, channel_config, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = channel_config[0]\n",
    "        self.conv1 = nn.Conv2d(in_channels, channel_config[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_config[0])\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i, num_blocks in enumerate(block_config):\n",
    "            s = 2 if i > 0 else 1\n",
    "            self.layers.append(self._make_layer(channel_config[i], num_blocks, stride=s))\n",
    "\n",
    "        self.linear = nn.Linear(channel_config[-1], num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResNetBlock(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "### --- MODIFIED: Corrected ModularCNN Constructor --- ###\n",
    "class ModularCNN(nn.Module):\n",
    "    def __init__(self, conv_channels, stride_config, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_ch_local = in_channels\n",
    "        \n",
    "        # Create a dictionary for easy lookup: {layer_index: stride_value}\n",
    "        stride_map = {idx: stride for idx, stride in stride_config}\n",
    "\n",
    "        for i in range(len(conv_channels)):\n",
    "            # Get the stride for the current layer, default to 1\n",
    "            s = stride_map.get(i, 1)\n",
    "            \n",
    "            # The stride is applied in the Conv2d layer itself for plain CNNs\n",
    "            conv_layer = nn.Conv2d(in_ch_local, conv_channels[i], kernel_size=3, stride=s, padding=1)\n",
    "            self.layers.append(nn.Sequential(conv_layer, nn.ReLU()))\n",
    "            in_ch_local = conv_channels[i]\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(conv_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ===================================================================\n",
    "# Part 2: Universal Model Engineering Function with NEW Blueprints\n",
    "# ===================================================================\n",
    "def generate_controlled_model(arch, design_type, target_value, control_variable, input_shape, num_classes=10, tolerance=0.1, max_iter=50):\n",
    "    print(f\"\\n--- Engineering '{arch.upper()}/{design_type.upper()}' (Controlling for {control_variable.upper()}) ---\")\n",
    "    if control_variable == 'macs': print(f\"Target MACs: {target_value / 1e6:.1f}M\")\n",
    "    else: print(f\"Target Params: {target_value / 1e6:.2f}M\")\n",
    "\n",
    "    base_channels = 24\n",
    "    \n",
    "    if arch == 'cnn':\n",
    "        num_layers = 6\n",
    "        strides_stable = [(1, 2), (3, 2), (5, 2)]\n",
    "        strides_erratic = [(2, 2), (5, 2)]\n",
    "        stride_config = strides_stable if design_type == 'stable' else strides_erratic\n",
    "    elif arch == 'resnet':\n",
    "        block_config_stable = [2, 2, 2, 2]  # ~ResNet-18\n",
    "        block_config_erratic = [4, 4, 4, 4] # ~ResNet-34\n",
    "        block_config = block_config_stable if design_type == 'stable' else block_config_erratic\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if arch == 'cnn':\n",
    "            channels = [int(base_channels * (1.5**i)) for i in range(num_layers)]\n",
    "            model = ModularCNN(conv_channels=channels, stride_config=stride_config, num_classes=num_classes, in_channels=input_shape[1])\n",
    "        elif arch == 'resnet':\n",
    "            channel_config = [int(base_channels), int(base_channels*2), int(base_channels*4), int(base_channels*8)]\n",
    "            model = ModularResNet(block_config, channel_config, num_classes=num_classes, in_channels=input_shape[1])\n",
    "\n",
    "        macs, params = profile(model.to('cpu'), inputs=(torch.randn(input_shape),), verbose=False)\n",
    "        current_value = macs if control_variable == 'macs' else params\n",
    "        \n",
    "        if abs(current_value - target_value) / target_value < tolerance:\n",
    "            print(f\"Success! Model has {macs/1e6:.1f}M MACs and {params/1e6:.2f}M params.\")\n",
    "            return model.to(device)\n",
    "        \n",
    "        ratio = np.sqrt(target_value / (current_value + 1e-9))\n",
    "        base_channels = max(4, int(base_channels * ratio))\n",
    "        \n",
    "    raise RuntimeError(f\"Failed to generate model for {arch}/{design_type} with target {target_value:.2e}\")\n",
    "\n",
    "# ===================================================================\n",
    "# Part 3: Training and Evaluation Infrastructure\n",
    "# ===================================================================\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "# ===================================================================\n",
    "# Part 4: Main Experimental Suite Runner\n",
    "# ===================================================================\n",
    "def run_experimental_suite(config):\n",
    "    set_random_seed(config['seed'])\n",
    "    \n",
    "    if config['dataset'] == 'cifar10':\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        val_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        input_shape, num_classes = (1, 3, 32, 32), 10\n",
    "    elif config['dataset'] == 'fashion_mnist':\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "        train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        val_ds = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        input_shape, num_classes = (1, 1, 28, 28), 10\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    model_a = generate_controlled_model(config['arch'], 'stable', config['target_params'], 'params', input_shape, num_classes)\n",
    "    model_c = generate_controlled_model(config['arch'], 'erratic', config['target_params'], 'params', input_shape, num_classes)\n",
    "    \n",
    "    models = {\"A: Stable (Low σ)\": model_a, \"C: Erratic (Params-Matched)\": model_c}\n",
    "    optimizers = {name: optim.Adam(model.parameters(), lr=config['lr']) for name, model in models.items()}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    results = {name: {'val_acc': []} for name in models.keys()}\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']} for {config['arch'].upper()}/{config['dataset'].upper()}\")\n",
    "        for name, model in models.items():\n",
    "            train_one_epoch(model, train_loader, criterion, optimizers[name], device)\n",
    "            _, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            results[name]['val_acc'].append(val_acc)\n",
    "            print(f\"  {name} - Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "    return results, models\n",
    "\n",
    "# ===================================================================\n",
    "# Part 5: Loss Landscape Visualization\n",
    "# ===================================================================\n",
    "def get_weights(model): return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "def set_weights(model, weights):\n",
    "    offset = 0\n",
    "    for p in model.parameters():\n",
    "        size = torch.prod(torch.tensor(p.shape)).item()\n",
    "        p.data.copy_(weights[offset:offset+size].view(p.shape))\n",
    "        offset += size\n",
    "\n",
    "def get_random_directions(model, device):\n",
    "    weights = [p.data for p in model.parameters()]\n",
    "    direction1 = [torch.randn_like(w, device=device) for w in weights]\n",
    "    norm1 = torch.sqrt(sum(torch.sum(d**2) for d in direction1))\n",
    "    for d in direction1: d.div_(norm1)\n",
    "    direction2 = [torch.randn_like(w, device=device) for w in weights]\n",
    "    dot_product = sum(torch.sum(d1 * d2) for d1, d2 in zip(direction1, direction2))\n",
    "    for d1, d2 in zip(direction1, direction2): d2.sub_(dot_product * d1)\n",
    "    norm2 = torch.sqrt(sum(torch.sum(d**2) for d in direction2))\n",
    "    for d in direction2: d.div_(norm2)\n",
    "    return direction1, direction2\n",
    "\n",
    "def calculate_loss_grid(model, directions, loader, criterion, device, grid_range=(-1, 1), grid_points=21):\n",
    "    w_star_flat = get_weights(model)\n",
    "    d1_flat = torch.cat([d.view(-1) for d in directions[0]])\n",
    "    d2_flat = torch.cat([d.view(-1) for d in directions[1]])\n",
    "    alphas, betas = np.linspace(*grid_range, grid_points), np.linspace(*grid_range, grid_points)\n",
    "    loss_grid = np.zeros((grid_points, grid_points))\n",
    "    temp_model = copy.deepcopy(model).to(device)\n",
    "    pbar = tqdm(total=grid_points*grid_points, desc=\"Scanning Landscape\", leave=False)\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        for j, beta in enumerate(betas):\n",
    "            set_weights(temp_model, w_star_flat + alpha * d1_flat + beta * d2_flat)\n",
    "            loss, _ = evaluate(temp_model, loader, criterion, device)\n",
    "            loss_grid[j, i] = loss\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return loss_grid, alphas, betas\n",
    "\n",
    "def plot_landscapes(models_to_visualize, val_loader, criterion, device, title):\n",
    "    num_models = len(models_to_visualize)\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(8 * num_models, 7), squeeze=False)\n",
    "    fig.suptitle(f'Loss Landscape Near Final Weights\\n({title})', fontsize=24, y=1.02)\n",
    "    for i, (name, model) in enumerate(models_to_visualize.items()):\n",
    "        ax = axes[0, i]\n",
    "        directions = get_random_directions(model, device)\n",
    "        loss_grid, alphas, betas = calculate_loss_grid(model, directions, val_loader, criterion, device)\n",
    "        contour = ax.contourf(alphas, betas, loss_grid, levels=20, cmap='viridis', norm=LogNorm())\n",
    "        fig.colorbar(contour, ax=ax, label='Validation Loss (Log Scale)')\n",
    "        ax.contour(alphas, betas, loss_grid, levels=20, colors='white', linewidths=0.5, alpha=0.5)\n",
    "        ax.plot(0, 0, 'r*', markersize=15, label='Final Weights (w*)')\n",
    "        ax.set_title(f\"Landscape for: {name}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Direction 1 (α)\"); ax.set_ylabel(\"Direction 2 (β)\")\n",
    "        ax.legend(); ax.set_aspect('equal', adjustable='box')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b5ecc-f27f-4c5b-a9df-412c99284863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T08:06:45.500681Z",
     "iopub.status.busy": "2025-07-01T08:06:45.499088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Engineering 'RESNET/STABLE' (Controlling for PARAMS) ---\n",
      "Target Params: 2.00M\n",
      "Success! Model has 100.3M MACs and 1.99M params.\n",
      "\n",
      "--- Engineering 'RESNET/ERRATIC' (Controlling for PARAMS) ---\n",
      "Target Params: 2.00M\n",
      "Success! Model has 93.3M MACs and 1.88M params.\n",
      "\n",
      "Epoch 1/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 66.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 58.39%\n",
      "\n",
      "Epoch 2/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 72.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 70.45%\n",
      "\n",
      "Epoch 3/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 76.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 71.15%\n",
      "\n",
      "Epoch 4/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 80.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 75.50%\n",
      "\n",
      "Epoch 5/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 80.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 74.55%\n",
      "\n",
      "Epoch 6/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 80.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 79.20%\n",
      "\n",
      "Epoch 7/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 78.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 74.99%\n",
      "\n",
      "Epoch 8/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 81.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 78.68%\n",
      "\n",
      "Epoch 9/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 80.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 80.21%\n",
      "\n",
      "Epoch 10/10 for RESNET/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 79.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 80.87%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Engineering 'CNN/STABLE' (Controlling for PARAMS) ---\n",
      "Target Params: 0.90M\n",
      "Success! Model has 45.6M MACs and 0.88M params.\n",
      "\n",
      "--- Engineering 'CNN/ERRATIC' (Controlling for PARAMS) ---\n",
      "Target Params: 0.90M\n",
      "Success! Model has 145.7M MACs and 0.88M params.\n",
      "\n",
      "Epoch 1/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 44.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 42.01%\n",
      "\n",
      "Epoch 2/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 53.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 53.14%\n",
      "\n",
      "Epoch 3/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 61.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 58.93%\n",
      "\n",
      "Epoch 4/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 63.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 62.63%\n",
      "\n",
      "Epoch 5/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 68.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 66.96%\n",
      "\n",
      "Epoch 6/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 70.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 69.90%\n",
      "\n",
      "Epoch 7/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 72.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 71.65%\n",
      "\n",
      "Epoch 8/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 74.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 73.94%\n",
      "\n",
      "Epoch 9/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 76.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C: Erratic (Params-Matched) - Val Acc: 72.30%\n",
      "\n",
      "Epoch 10/10 for CNN/CIFAR10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A: Stable (Low σ) - Val Acc: 77.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 250/391 [00:03<00:01, 74.29it/s]"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Part 6: Main Execution and Visualization\n",
    "# ===================================================================\n",
    "all_results = {}\n",
    "\n",
    "### --- MODIFIED: Adjusted parameter budgets for feasibility --- ###\n",
    "experiment_configs = [\n",
    "    {'arch': 'resnet', 'dataset': 'cifar10', 'target_params': 2000000},       # 2.0M params\n",
    "    {'arch': 'cnn', 'dataset': 'cifar10', 'target_params': 900000},          # 0.9M params (Increased)\n",
    "    {'arch': 'resnet', 'dataset': 'fashion_mnist', 'target_params': 1500000}, # 1.5M params\n",
    "    {'arch': 'cnn', 'dataset': 'fashion_mnist', 'target_params': 400000},      # 0.4M params (Increased)\n",
    "]\n",
    "\n",
    "base_config = {'seed': 42, 'epochs': 10, 'lr': 1e-3, 'batch_size': 128}\n",
    "\n",
    "# --- Run all experiments ---\n",
    "for exp_config in experiment_configs:\n",
    "    config = {**base_config, **exp_config}\n",
    "    key = (config['arch'], config['dataset'])\n",
    "    try:\n",
    "        results, trained_models = run_experimental_suite(config)\n",
    "        all_results[key] = (results, trained_models)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n!!!!!! SKIPPING EXPERIMENT {key} due to error: {e} !!!!!!\\n\")\n",
    "        all_results[key] = (None, None)\n",
    "\n",
    "\n",
    "# --- Plot training curves ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharey=True)\n",
    "fig.suptitle('Training Efficiency: The Impact of Architectural Shape (RG Flow)', fontsize=24, y=0.95)\n",
    "\n",
    "plotted_count = 0\n",
    "for i, config in enumerate(experiment_configs):\n",
    "    key = (config['arch'], config['dataset'])\n",
    "    if key in all_results and all_results[key][0] is not None:\n",
    "        results, _ = all_results[key]\n",
    "        arch, dataset = key\n",
    "        ax = axes[plotted_count//2, plotted_count%2]\n",
    "        colors = ['#1f77b4', '#d62728'] # Blue for Stable, Red for Erratic\n",
    "        for j, (name, metrics) in enumerate(results.items()):\n",
    "            label_name = name.split(\":\")[1].strip().replace(\" (Params-Matched)\",\"\")\n",
    "            ax.plot(metrics['val_acc'], label=label_name, color=colors[j], marker='o', markersize=4, linestyle='--')\n",
    "        ax.set_title(f\"{arch.upper()} on {dataset.replace('_', ' ').title()}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Validation Accuracy (%)\")\n",
    "        ax.legend(fontsize=12); ax.grid(True)\n",
    "        plotted_count += 1\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93]); plt.show()\n",
    "\n",
    "# --- Plot the most illustrative loss landscape ---\n",
    "print(\"\\n--- Visualizing key loss landscape for ResNet on CIFAR-10 ---\")\n",
    "key_to_plot = ('resnet', 'cifar10')\n",
    "if key_to_plot in all_results and all_results[key_to_plot][1] is not None:\n",
    "    _, trained_models = all_results[key_to_plot]\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    val_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    val_loader = DataLoader(val_ds, batch_size=512, shuffle=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    plot_landscapes(trained_models, val_loader, criterion, device, title=\"ResNet on CIFAR-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e80db-65a6-4552-acab-591d5ce31a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
