{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e2047a-91d2-4497-8f30-2bb2130652da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T09:05:01.113710Z",
     "iopub.status.busy": "2025-06-09T09:05:01.113476Z",
     "iopub.status.idle": "2025-06-09T09:05:01.199306Z",
     "shell.execute_reply": "2025-06-09T09:05:01.198890Z",
     "shell.execute_reply.started": "2025-06-09T09:05:01.113694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    module=\"scipy.optimize._minpack_py\"\n",
    ")\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# === Utils ===\n",
    "\n",
    "_ds_cache = {}\n",
    "\n",
    "def get_data(ds_name: str, resize: int):\n",
    "    \"\"\"\n",
    "    Return a torchvision Dataset for `ds_name` ('cifar' or 'mnist'),\n",
    "    resized to (resize × resize).  Downloads only on first call.\n",
    "    \"\"\"\n",
    "    key = (ds_name, resize)\n",
    "    if key not in _ds_cache:\n",
    "        # pick the right class\n",
    "        DataClass = datasets.CIFAR10 if ds_name == 'cifar' else datasets.MNIST\n",
    "\n",
    "        # build & stash it\n",
    "        tf = transforms.Compose([\n",
    "            transforms.Resize((resize, resize)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        _ds_cache[key] = DataClass(\n",
    "            './data',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=tf\n",
    "        )\n",
    "    return _ds_cache[key]\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def summarize_sd(arr):\n",
    "    arr = np.asarray(arr, float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return \"n/a\"\n",
    "    return f\"{arr.mean():.3f} ± {arr.std():.3f} (SD, n={len(arr)})\"\n",
    "\n",
    "def choose_summary(key, arr):\n",
    "    if key in {\"global_pvalue\", \"mean_mi_pval\"}:\n",
    "        return summarize_p(arr)\n",
    "    if key == \"mean_mi_std_lambda\":\n",
    "        return summarize_sd(arr)\n",
    "    # default → 95 % CI\n",
    "    return summarize_ci(arr)\n",
    "\n",
    "def summarize_ci(arr, alpha=0.05):\n",
    "    \"\"\"\n",
    "    mean ± 95 % CI (Student-t, two-sided).\n",
    "    Returns 'n/a' if nothing to summarise.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    n   = len(arr)\n",
    "    if n == 0:\n",
    "        return \"n/a\"\n",
    "    mean = arr.mean()\n",
    "    sem  = stats.sem(arr)               \n",
    "    half = stats.t.ppf(1-alpha/2, n-1) * sem\n",
    "    return f\"{mean:.3f} ± {half:.3f} (95 % CI, n={n})\"\n",
    "\n",
    "def summarize_p(arr):\n",
    "    \"\"\"median [IQR] for p-values – unchanged.\"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return \"n/a\"\n",
    "    q25, q75 = np.percentile(arr, [25, 75])\n",
    "    return f\"{np.median(arr):.3f} [IQR {q25:.3f}–{q75:.3f}] (n={len(arr)})\"\n",
    "\n",
    "# === Models ===\n",
    "class ModularCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_channels,\n",
    "        kernel_sizes,\n",
    "        strides,\n",
    "        use_leaky_relu: bool = False,\n",
    "        use_batchnorm: bool = False,\n",
    "        in_channels: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList() if use_batchnorm else None\n",
    "        self.use_bn = use_batchnorm\n",
    "        self.strides = strides\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.use_leaky = use_leaky_relu\n",
    "        for out_ch, k, s in zip(conv_channels, kernel_sizes, strides):\n",
    "            conv = nn.Conv2d(in_channels, out_ch, kernel_size=k, padding=k//2, bias=True)\n",
    "            self.convs.append(conv)\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm2d(out_ch))\n",
    "            in_channels = out_ch\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if self.use_bn:\n",
    "                x = self.bns[idx](x)\n",
    "            x = F.leaky_relu(x, 0.01) if self.use_leaky else F.relu(x)\n",
    "            activations.append(x.clone())\n",
    "            s = self.strides[idx]\n",
    "            x = F.avg_pool2d(x, kernel_size=s)\n",
    "        return activations\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        out_ch: int,\n",
    "        stride: int = 1,\n",
    "        kernel_size: int = 3,\n",
    "        bottleneck_ratio: float = 1.0,\n",
    "        projection_type: str = 'identity',\n",
    "        activation: str = 'relu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mid_ch = int(out_ch * bottleneck_ratio)\n",
    "        # 1st conv\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size, stride, kernel_size//2, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(mid_ch)\n",
    "        # 2nd conv\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size, 1, kernel_size//2, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        # shortcut\n",
    "        if projection_type == 'conv1x1' or stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        # activation\n",
    "        if activation == 'leaky_relu':\n",
    "            self.act = lambda x: F.leaky_relu(x, 0.01)\n",
    "        else:\n",
    "            self.act = F.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y = y + self.shortcut(x)\n",
    "        return self.act(y)\n",
    "\n",
    "class ModularResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        block_sizes,\n",
    "        kernel_sizes,\n",
    "        strides,\n",
    "        bottleneck_ratios,\n",
    "        projection_types,\n",
    "        activation_functions,\n",
    "        in_channels: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # stem\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels[0])\n",
    "        self.act1  = F.relu\n",
    "        self.in_ch = channels[0]\n",
    "        # store params\n",
    "        self.channels = channels\n",
    "        self.block_sizes = block_sizes\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.strides_list = strides\n",
    "        self.bottleneck_ratios = bottleneck_ratios\n",
    "        self.projection_types = projection_types\n",
    "        self.activation_functions = activation_functions\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(len(channels)):\n",
    "            self.layers.append(self._make_layer(\n",
    "                out_ch = channels[idx],\n",
    "                num_blocks = block_sizes[idx],\n",
    "                stride = strides[idx],\n",
    "                kernel_size = kernel_sizes[idx],\n",
    "                bottleneck_ratio = bottleneck_ratios[idx],\n",
    "                projection_type = projection_types[idx],\n",
    "                activation = activation_functions[idx]\n",
    "            ))\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        out_ch,\n",
    "        num_blocks,\n",
    "        stride,\n",
    "        kernel_size,\n",
    "        bottleneck_ratio,\n",
    "        projection_type,\n",
    "        activation\n",
    "    ):\n",
    "        blocks = []\n",
    "        # first block\n",
    "        blocks.append(ResNetBlock(\n",
    "            self.in_ch, out_ch, stride,\n",
    "            kernel_size, bottleneck_ratio,\n",
    "            projection_type, activation\n",
    "        ))\n",
    "        self.in_ch = out_ch\n",
    "        # rest\n",
    "        for _ in range(1, num_blocks):\n",
    "            blocks.append(ResNetBlock(\n",
    "                self.in_ch, out_ch, 1,\n",
    "                kernel_size, bottleneck_ratio,\n",
    "                'identity', activation\n",
    "            ))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        x = self.act1(self.bn1(self.conv1(x)))\n",
    "        acts.append(x.clone())\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            acts.append(x.clone())\n",
    "        return acts\n",
    "\n",
    "# === Architecture Generators ===\n",
    "def generate_random_cnn(\n",
    "    n_layers: int,\n",
    "    stride_choices,\n",
    "    kernel_choices,\n",
    "    channel_choices,\n",
    "    input_size: int,\n",
    "    in_channels: int,\n",
    "    use_leaky_relu: bool,\n",
    "    use_batchnorm: bool\n",
    "):\n",
    "    size = input_size\n",
    "    channels, kernels, strides = [], [], []\n",
    "    for _ in range(n_layers):\n",
    "        s = random.choice([st for st in stride_choices if size >= 2*st] or [1])\n",
    "        k = random.choice(kernel_choices)\n",
    "        c = random.choice(channel_choices)\n",
    "        strides.append(s)\n",
    "        kernels.append(k)\n",
    "        channels.append(c)\n",
    "        size = max(1, size // s)\n",
    "    return ModularCNN(channels, kernels, strides, use_leaky_relu, use_batchnorm, in_channels)\n",
    "\n",
    "def generate_random_resnet(\n",
    "    n_layers: int,\n",
    "    channel_choices,\n",
    "    input_size: int,\n",
    "    in_channels: int,\n",
    "    block_sizes,\n",
    "    kernel_sizes,\n",
    "    stride_choices,\n",
    "    bottleneck_ratios,\n",
    "    projection_types,\n",
    "    activation_functions\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a random ResNet with n_layers blocks. For each block we sample:\n",
    "      - out_channels from channel_choices\n",
    "      - num_blocks from block_sizes\n",
    "      - kernel_size from kernel_sizes\n",
    "      - stride from stride_choices (only if spatial size allows)\n",
    "      - bottleneck_ratio from bottleneck_ratios\n",
    "      - projection_type from projection_types\n",
    "      - activation from activation_functions\n",
    "    \"\"\"\n",
    "    size = input_size\n",
    "    channels, blocks, ks, strides, brs, pts, afs = [], [], [], [], [], [], []\n",
    "\n",
    "    # Sample up to n_layers (stop early if spatial collapses)\n",
    "    for _ in range(n_layers):\n",
    "        c  = random.choice(channel_choices)\n",
    "        b  = random.choice(block_sizes)\n",
    "        k  = random.choice(kernel_sizes)\n",
    "        br = random.choice(bottleneck_ratios)\n",
    "        pt = random.choice(projection_types)\n",
    "        af = random.choice(activation_functions)\n",
    "\n",
    "        # only allow stride>1 if size>=2*stride\n",
    "        possible_strides = [s for s in stride_choices if size >= 2*s]\n",
    "        s = random.choice(possible_strides) if possible_strides else 1\n",
    "\n",
    "        channels.append(c)\n",
    "        blocks.append(b)\n",
    "        ks.append(k)\n",
    "        strides.append(s)\n",
    "        brs.append(br)\n",
    "        pts.append(pt)\n",
    "        afs.append(af)\n",
    "\n",
    "        size = max(1, size // s)\n",
    "        if size <= 1:\n",
    "            break\n",
    "\n",
    "    return ModularResNet(\n",
    "        channels       = channels,\n",
    "        block_sizes    = blocks,\n",
    "        kernel_sizes   = ks,\n",
    "        strides        = strides,\n",
    "        bottleneck_ratios   = brs,\n",
    "        projection_types    = pts,\n",
    "        activation_functions= afs,\n",
    "        in_channels    = in_channels\n",
    "    )\n",
    "\n",
    "# === Property Extraction ===\n",
    "def extract_cnn_properties(model: ModularCNN):\n",
    "    strides, kernels, channels, depths, overlaps = [], [], [], [], []\n",
    "    for i, conv in enumerate(model.convs):\n",
    "        k = model.kernel_sizes[i]\n",
    "        s = model.strides[i]\n",
    "        c = conv.out_channels\n",
    "        strides.append(s)\n",
    "        kernels.append(k)\n",
    "        channels.append(c)\n",
    "        depths.append(i+1)\n",
    "        overlaps.append((k - s) / k)\n",
    "    return {\n",
    "        's': np.array(strides),\n",
    "        'r': np.array(kernels),\n",
    "        'c': np.array(channels),\n",
    "        'inv_c': 1/np.array(channels),\n",
    "        'd': np.array(depths),\n",
    "        'o': np.array(overlaps),\n",
    "        's_r': np.array([s*r for s,r in zip(strides, kernels)])\n",
    "    }\n",
    "\n",
    "def extract_resnet_properties(model: ModularResNet):\n",
    "    ks = model.kernel_sizes\n",
    "    ss = model.strides_list\n",
    "    br = model.bottleneck_ratios\n",
    "    pt = model.projection_types\n",
    "    af = model.activation_functions\n",
    "    skip_width, block_depth, layer_index, channels, res_type = [], [], [], [], []\n",
    "    kernel_size, stride, bottleneck, projection, activation, depths = [], [], [], [], [], []\n",
    "\n",
    "    # initial conv\n",
    "    layer = 1\n",
    "    channels.append(model.conv1.out_channels)\n",
    "    skip_width.append(0)\n",
    "    block_depth.append(0)\n",
    "    layer_index.append(layer)\n",
    "    res_type.append(0)\n",
    "    kernel_size.append(3)\n",
    "    stride.append(1)\n",
    "    bottleneck.append(1.0)\n",
    "    projection.append(0)\n",
    "    activation.append(0)\n",
    "    depths.append(layer)\n",
    "    layer += 1\n",
    "\n",
    "    for li, seq in enumerate(model.layers):\n",
    "        for bj, blk in enumerate(seq):\n",
    "            in_ch = blk.conv1.in_channels\n",
    "            out_ch = blk.conv2.out_channels\n",
    "            has_proj = not isinstance(blk.shortcut, nn.Identity)\n",
    "\n",
    "            skip_width.append(out_ch - in_ch if has_proj else 0)\n",
    "            block_depth.append(bj)\n",
    "            layer_index.append(layer)\n",
    "            res_type.append(1 if has_proj else 0)\n",
    "\n",
    "            if bj == 0:\n",
    "                kernel_size.append(ks[li])\n",
    "                stride.append(ss[li])\n",
    "                bottleneck.append(br[li])\n",
    "                projection.append(1 if pt[li] != 'identity' else 0)\n",
    "                activation.append(1 if af[li]=='leaky_relu' else 0)\n",
    "            else:\n",
    "                kernel_size.append(ks[li])\n",
    "                stride.append(1)\n",
    "                bottleneck.append(br[li])\n",
    "                projection.append(0)\n",
    "                activation.append(1 if af[li]=='leaky_relu' else 0)\n",
    "\n",
    "            channels.append(out_ch)\n",
    "            depths.append(layer)\n",
    "            layer += 1\n",
    "\n",
    "    return {\n",
    "        'skip_width': np.array(skip_width),\n",
    "        'block_depth': np.array(block_depth),\n",
    "        'layer_index': np.array(layer_index),\n",
    "        'inv_c': 1/np.array(channels),\n",
    "        'res_type': np.array(res_type),\n",
    "        'kernel_size': np.array(kernel_size),\n",
    "        'stride': np.array(stride),\n",
    "        'bottleneck': np.array(bottleneck),\n",
    "        'projection': np.array(projection),\n",
    "        'activation': np.array(activation),\n",
    "        'd': np.array(depths)\n",
    "    }\n",
    "\n",
    "def extract_model_properties(model):\n",
    "    if isinstance(model, ModularCNN):\n",
    "        return extract_cnn_properties(model)\n",
    "    elif isinstance(model, ModularResNet):\n",
    "        return extract_resnet_properties(model)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# === Metrics ===\n",
    "\n",
    "def compute_lambda_i_empirical(model, x, noise_std=0.2, eps=1e-8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        noisy = x + noise_std * torch.randn_like(x)\n",
    "        clean_acts = model(x)\n",
    "        noisy_acts = model(noisy)\n",
    "\n",
    "    # stack → (L, …), flatten each activation, compute norms in one go\n",
    "    deltas = torch.stack(\n",
    "        [(n - c).flatten(1).norm(dim=1) for n, c in zip(noisy_acts, clean_acts)]\n",
    "    ).squeeze()                       # shape (L,)\n",
    "\n",
    "    lam = -torch.log((deltas[1:] + eps) / (deltas[:-1] + eps))\n",
    "    return lam.cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_input_sensitivity(model, x):\n",
    "    model.eval()\n",
    "    xv = x.clone().requires_grad_(True)\n",
    "    acts = model(xv)\n",
    "\n",
    "    input_sens = []\n",
    "    for act in acts:\n",
    "        # exactly the same mathematical expression as before\n",
    "        loss = act.square().sum()\n",
    "        (grad,) = torch.autograd.grad(loss, xv, retain_graph=True)\n",
    "        input_sens.append(grad.norm().item())\n",
    "\n",
    "    return input_sens\n",
    "\n",
    "def mi_permutation_test(X, y, n_perm=200):\n",
    "    n_neighbors = max(1, min(2, len(y) - 1))  \n",
    "    actual_mi = mutual_info_regression(X, y, random_state=0, n_neighbors=n_neighbors)[0]\n",
    "    perm_mis = []\n",
    "    for _ in range(n_perm):\n",
    "        y_perm = np.random.permutation(y)\n",
    "        perm_mi = mutual_info_regression(X, y_perm, random_state=0, n_neighbors=n_neighbors)[0]\n",
    "        perm_mis.append(perm_mi)\n",
    "    pval = (np.sum(np.array(perm_mis) >= actual_mi) + 1) / (n_perm + 1)\n",
    "    return actual_mi, pval\n",
    "\n",
    "# === Regression & Analysis ===\n",
    "def create_feature_matrix(raw, config):\n",
    "    X, names, powers = [], [], []\n",
    "    used = set()\n",
    "    for c in config:\n",
    "        name = c['name']\n",
    "        if name not in raw or name in used:\n",
    "            continue\n",
    "        f = raw[name].astype(float).copy()\n",
    "        used.add(name)\n",
    "        if c.get('log', False):\n",
    "            f = np.log1p(np.clip(f, 0, None))\n",
    "        if c.get('power_law', False) or c.get('normalize', False):\n",
    "            f = (f - f.min())/(f.max()-f.min()+1e-8)\n",
    "        X.append(f)\n",
    "        names.append(name)\n",
    "        powers.append(c.get('power_law', False))\n",
    "    return np.vstack(X), names, powers\n",
    "\n",
    "def fit_model(X, y, powers, n_boot=100):\n",
    "    import warnings\n",
    "    from scipy.optimize import OptimizeWarning\n",
    "\n",
    "    def model_fn(x, *p):\n",
    "        total = np.zeros_like(x[0], dtype=float)\n",
    "        idx = 0\n",
    "        for j, pl in enumerate(powers):\n",
    "            a = p[idx]\n",
    "            if pl:\n",
    "                b = p[idx+1]\n",
    "                xj = x[j] + 1e-12\n",
    "                # Safely calculate power term\n",
    "                with np.errstate(over='ignore', invalid='ignore'):\n",
    "                    log_xj = np.log(xj)\n",
    "                    # Clip to avoid overflow in exp()\n",
    "                    power = np.clip(b * log_xj, -500, 500)\n",
    "                    term = np.exp(power)\n",
    "                    # Clean up any invalid values\n",
    "                    term = np.nan_to_num(term, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                total += a * term\n",
    "                idx += 2\n",
    "            else:\n",
    "                total += a * x[j]\n",
    "                idx += 1\n",
    "        total += p[idx]\n",
    "        return total\n",
    "\n",
    "    p0 = []\n",
    "    for pl in powers:\n",
    "        p0 += [0.1, 1.0] if pl else [0.1]\n",
    "    p0 += [1.0]\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", OptimizeWarning)\n",
    "        popt, _ = curve_fit(model_fn, X, y, p0=p0, maxfev=10000)\n",
    "\n",
    "    pred = model_fn(X, *popt)\n",
    "    rmse = np.sqrt(((y-pred)**2).mean())\n",
    "    r2 = linregress(y, pred).rvalue**2\n",
    "\n",
    "    samples = []\n",
    "    for _ in tqdm(range(n_boot), desc=\"Bootstrapping\", leave=False):\n",
    "        idxs = np.random.choice(len(y), len(y), replace=True)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", OptimizeWarning)\n",
    "                pp, _ = curve_fit(model_fn, X[:,idxs], y[idxs], p0=p0, maxfev=5000)\n",
    "            samples.append(pp)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    param_stds = np.std(np.stack(samples), axis=0) if samples else np.zeros_like(popt)\n",
    "    return {'popt': popt, 'param_stds': param_stds, 'pred': pred, 'rmse': rmse, 'r2': r2, 'model_fn': model_fn}\n",
    "\n",
    "\n",
    "\n",
    "def plot_key_results(results, title_suffix=\"\"):\n",
    "    # 1) Input Sensitivity vs Depth\n",
    "    grouped = results['grouped']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(grouped['depth'], grouped['mean'], yerr=grouped['std'], fmt='o-', capsize=3)\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.ylabel(\"Mean Input Sensitivity\")\n",
    "    plt.title(f\"Input Sensitivity vs Depth{title_suffix}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2) σ² vs 1/(L–1) Regression\n",
    "    inv   = results['inv_Lminus1']      # array of 1/(L_i - 1)\n",
    "    sigma2 = results['sigma2']          # array of σ_i²\n",
    "    slope, intercept, r_value, p_value, stderr = linregress(inv, sigma2)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(inv, sigma2, alpha=0.5, label=\"data\")\n",
    "    xs = np.linspace(inv.min(), inv.max(), 200)\n",
    "    plt.plot(xs, intercept + slope*xs, 'r--',\n",
    "             label=f\"fit: y={slope:.2e}x+{intercept:.2e}\\n$R^2$={r_value**2:.2f}\")\n",
    "    plt.xlabel(\"1 / (L - 1)\")\n",
    "    plt.ylabel(\"σ²\")\n",
    "    plt.title(f\"σ² vs 1/(L–1){title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_global_complexity(raw_props, n_components=1):\n",
    "    \"\"\"\n",
    "    Compute a 1D \"ACS\" per model as the projection onto the first principal component\n",
    "    of the global (minmax‐scaled) parameter space.\n",
    "    \"\"\"\n",
    "    # 1) Stack parameters into (n_models, n_features)\n",
    "    keys = list(raw_props.keys())\n",
    "    X = np.vstack([ raw_props[k] for k in keys ]).T  # shape (n_models, n_keys)\n",
    "\n",
    "    # 2) Min–max scale each column to [0,1]\n",
    "    mins = X.min(axis=0, keepdims=True)\n",
    "    maxs = X.max(axis=0, keepdims=True)\n",
    "    X_norm = (X - mins) / (maxs - mins)\n",
    "\n",
    "    # 3) PCA to 1 component\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(X_norm)  # shape (n_models, 1)\n",
    "\n",
    "    # 4) Return the first principal component (flattened)\n",
    "    return pcs[:, 0]\n",
    "\n",
    "def classify_models(complexity_scores, percentile=10):\n",
    "    sorted_idx = np.argsort(complexity_scores)\n",
    "    n = len(complexity_scores)\n",
    "    labels = np.array(['unlabeled'] * n)\n",
    "    k = max(1, int(n * percentile / 100))\n",
    "    labels[sorted_idx[:k]] = 'simple'\n",
    "    labels[sorted_idx[-k:]] = 'complex'\n",
    "    return labels\n",
    "\n",
    "def bin_and_validate(input_sens_means, complexity_labels, n_bins=10):\n",
    "    bins = np.linspace(min(input_sens_means), max(input_sens_means), n_bins + 1)\n",
    "    bin_indices = np.digitize(input_sens_means, bins) - 1\n",
    "    valid_bins = []\n",
    "    for bin_id in range(n_bins):\n",
    "        idx = np.where(bin_indices == bin_id)[0]\n",
    "        labels_in_bin = complexity_labels[idx]\n",
    "        simple_count = np.sum(labels_in_bin == 'simple')\n",
    "        complex_count = np.sum(labels_in_bin == 'complex')\n",
    "        if len(idx) >= 5 and simple_count >= 3 and complex_count >= 3:\n",
    "            valid_bins.append({\n",
    "                'indices': idx,\n",
    "                'simple_count': simple_count,\n",
    "                'complex_count': complex_count,\n",
    "                'bin_id': bin_id\n",
    "            })\n",
    "    return valid_bins\n",
    "\n",
    "def compute_bin_metrics(valid_bins, lambda_vals, complexity_scores, raw_props, complexity_labels):\n",
    "    \"\"\"\n",
    "    For each valid FI‐bin, compute:\n",
    "      - CV of σ\n",
    "      - Mutual Information on ACS and on all_params (with p‐values)\n",
    "      - σ(simple)/σ(complex) ratio and its KS‐test p‐value\n",
    "    Returns a pandas.DataFrame with columns:\n",
    "      ['IS_bin', '#simple', '#complex', 'CV',\n",
    "       'ratio', 'pval_ratio',\n",
    "       'MI_ACS', 'MI_pval_ACS',\n",
    "       'MI_all_params', 'MI_pval_all_params']\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    for b in valid_bins:\n",
    "        idx = b['indices']\n",
    "\n",
    "        # compute σ for all models in this bin\n",
    "        sigma_l = np.array([np.std(lambda_vals[i]) for i in idx])\n",
    "\n",
    "        # coefficient of variation\n",
    "        CV = sigma_l.std() / (sigma_l.mean() + 1e-12)\n",
    "\n",
    "        # --- simple vs complex σ ---\n",
    "        lam_s = [np.std(lambda_vals[i]) for i in idx if complexity_labels[i] == 'simple']\n",
    "        lam_c = [np.std(lambda_vals[i]) for i in idx if complexity_labels[i] == 'complex']\n",
    "        # KS‐test on the two distributions\n",
    "        _, p_val = ks_2samp(lam_s, lam_c)\n",
    "        # ratio of means\n",
    "        ratio = np.nanmean(lam_s) / np.nanmean(lam_c)\n",
    "\n",
    "        # mutual information tests\n",
    "        acs         = complexity_scores[idx].reshape(-1, 1)\n",
    "        all_params  = np.array([[raw_props[k][i] for k in raw_props] for i in idx])\n",
    "        mi_acs, p_acs       = mi_permutation_test(acs, sigma_l)\n",
    "        mi_params, p_params = mi_permutation_test(all_params, sigma_l)\n",
    "\n",
    "        metrics.append({\n",
    "            'IS_bin':            b['bin_id'],\n",
    "            '#simple':           b['simple_count'],\n",
    "            '#complex':          b['complex_count'],\n",
    "            'CV':                CV,\n",
    "            'ratio':             ratio,\n",
    "            'pval_ratio':        p_val,\n",
    "            'MI_ACS':            mi_acs,\n",
    "            'MI_pval_ACS':       p_acs,\n",
    "            'MI_all_params':     mi_params,\n",
    "            'MI_pval_all_params': p_params\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# === Experiment Runner ===\n",
    "def run_experiment(cfg):\n",
    "    # ───────────────────────────── SET-UP ──────────────────────────────\n",
    "    set_random_seed(cfg['seed'])\n",
    "    ds = cfg['dataset'].lower()\n",
    "    base, ch = (32, 3) if ds == 'cifar' else (28, 1)\n",
    "    scale = random.uniform(*cfg['resize_range'])\n",
    "    tgt = int(base * scale)\n",
    "\n",
    "    print(f\"Running run_id       : {cfg['run_id']}\")\n",
    "    print(f\"\\nResize scale factor  : {scale:.2f}  →  target size {tgt}\")\n",
    "\n",
    "    data = get_data(ds, tgt)\n",
    "    n = max(1, int(len(data) * cfg['data_fraction']))\n",
    "    idx = np.random.choice(len(data), n, replace=False)\n",
    "    samples = [data[i][0] for i in idx]\n",
    "    x = torch.stack(samples, dim=0).to(device)\n",
    "    noise_std = cfg['noise_std']\n",
    "    input_size, in_channels = tgt, ch\n",
    "\n",
    "    print(f\"Input shape          : {tuple(x.shape)}\")\n",
    "\n",
    "    # ───────────────────── GENERATE & MEASURE MODELS ────────────────────\n",
    "    data_pts, input_sens_vals, lambda_vals = [], [], []\n",
    "    lambda_stds, model_depths          = [], []\n",
    "\n",
    "    def _measure_model(model):\n",
    "        with autocast():\n",
    "            lam_raw = compute_lambda_i_empirical(model, x, noise_std)\n",
    "        lam = lam_raw.mean(axis=1) if lam_raw.ndim > 1 else lam_raw\n",
    "        if len(lam) < 3:\n",
    "            return None\n",
    "        with autocast(enabled=False):\n",
    "            input_sens = compute_input_sensitivity(model, x)\n",
    "        props = extract_model_properties(model)\n",
    "        ml = min(len(lam)+1, len(input_sens))\n",
    "        for k in props:\n",
    "            props[k] = props[k][:ml]\n",
    "        rows = []\n",
    "        for j in range(ml-1):\n",
    "            rows.append({**{k: props[k][j] for k in props}, 'empirical': float(lam[j])})\n",
    "        return lam, input_sens, rows\n",
    "\n",
    "    for _ in tqdm(range(cfg['n_models']), desc=\"Measuring models\"):\n",
    "        depth = random.choice(cfg['depth_choices'])\n",
    "        if cfg['architecture'] == 'cnn':\n",
    "            model = generate_random_cnn(\n",
    "                depth,\n",
    "                cfg['param_ranges']['strides'],\n",
    "                cfg['param_ranges']['kernels'],\n",
    "                cfg['param_ranges']['channels'],\n",
    "                input_size, in_channels,\n",
    "                cfg.get('use_leaky_relu', True),\n",
    "                cfg.get('use_batchnorm', False)\n",
    "            ).to(device)\n",
    "        else:\n",
    "            pr = cfg['param_ranges']\n",
    "            model = generate_random_resnet(\n",
    "                depth, pr['channels'], input_size, in_channels,\n",
    "                pr['block_sizes'], pr['kernel_sizes'], pr['strides'],\n",
    "                pr['bottleneck_ratios'], pr['projection_types'],\n",
    "                pr['activation_functions']\n",
    "            ).to(device)\n",
    "        res = _measure_model(model)\n",
    "        del model\n",
    "        if res is None:\n",
    "            continue\n",
    "        lam, input_sens, rows = res\n",
    "        lambda_stds.append(float(np.std(lam)))\n",
    "        model_depths.append(len(lam) + 1)\n",
    "        input_sens_vals.append(input_sens[:-1])\n",
    "        lambda_vals.append(lam)\n",
    "        data_pts.extend(rows)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if not data_pts:\n",
    "        print(\"No valid data collected for this run.\")\n",
    "        return {\n",
    "            'run_id': cfg['run_id'],\n",
    "            'architecture': cfg['architecture'],\n",
    "            'dataset': cfg['dataset']\n",
    "        }\n",
    "\n",
    "    n_models_valid = len(lambda_stds)\n",
    "    print(f\"Generated models accepted       : {n_models_valid}\")\n",
    "    print(f\"Unique datapoints               : {len(set(tuple(d.items()) for d in data_pts))}\")\n",
    "\n",
    "    df          = pd.DataFrame(data_pts)\n",
    "    raw_props   = {c: df[c].values for c in df.columns if c != 'empirical'}\n",
    "    empirical   = df['empirical'].values\n",
    "    input_sens_flat = np.concatenate(input_sens_vals)\n",
    "    lambda_flat = np.concatenate(lambda_vals)\n",
    "\n",
    "    complexity_scores = compute_global_complexity(raw_props)\n",
    "    percentile = cfg.get(\"complexity_percentile\", 10)\n",
    "    complexity_labels = classify_models(complexity_scores, percentile)\n",
    "\n",
    "    input_sens_means = np.array([np.mean(f) for f in input_sens_vals])\n",
    "    valid_bins = bin_and_validate(input_sens_means, complexity_labels)\n",
    "\n",
    "    per_bin_metrics = compute_bin_metrics(valid_bins, lambda_vals, complexity_scores, raw_props, complexity_labels)\n",
    "    \n",
    "    # ──────────────────────────── Compute global σ(simple)/σ(complex) ────────────────────────────\n",
    "    ratios = []\n",
    "    pvals  = []\n",
    "    for b in valid_bins:\n",
    "       # collect σ for simple vs complex in this bin\n",
    "       lam_s = [ np.std(lambda_vals[i]) \n",
    "                 for i in b['indices'] \n",
    "                 if complexity_labels[i]=='simple' ]\n",
    "       lam_c = [ np.std(lambda_vals[i]) \n",
    "                 for i in b['indices'] \n",
    "                 if complexity_labels[i]=='complex' ]\n",
    "       # ks_2samp returns (statistic, p)\n",
    "       _, p = ks_2samp(lam_s, lam_c)\n",
    "       ratios.append(np.nanmean(lam_s) / np.nanmean(lam_c))\n",
    "       pvals .append(p)\n",
    "\n",
    "    # overall mean ratio and median p‐value across all bins\n",
    "    mean_ratio = np.nanmean(ratios)\n",
    "    median_p   = np.nanmedian(pvals)\n",
    "\n",
    "    # ───────── Group by depth (mean & std) and fit slope ────────────\n",
    "    slope_input_sens = r_value_input_sens = np.nan\n",
    "    grp = None\n",
    "    try:\n",
    "        grp = (\n",
    "            pd.DataFrame({\n",
    "                'depth': raw_props['d'],\n",
    "                'input_sens': input_sens_flat\n",
    "            })\n",
    "            .groupby('depth')['input_sens']\n",
    "            .agg(['mean','std'])\n",
    "            .reset_index()\n",
    "        )\n",
    "        # now do the log‐log fit on mean\n",
    "        log_d2 = np.log(grp['depth'] + 1e-8)\n",
    "        log_f2 = np.log(grp['mean']  + 1e-8)\n",
    "        slope_input_sens, _, r_value_input_sens, _, _ = linregress(log_d2, log_f2)\n",
    "    except Exception:\n",
    "        grp = None\n",
    "\n",
    "    print(\"\\nIS Scaling with Depth\")\n",
    "    if not np.isnan(slope_input_sens):\n",
    "        print(f\"  IS   ~ depth^{slope_input_sens:.3f}   (R² = {r_value_input_sens**2:.3f})\")\n",
    "        \n",
    "    if valid_bins:\n",
    "        # compute per‐bin means\n",
    "        bin_centers = [np.mean([input_sens_means[i] for i in b['indices']]) for b in valid_bins]\n",
    "        all_complexities = [complexity_scores[i] for b in valid_bins for i in b['indices']]\n",
    "        all_sigmas      = [np.std(lambda_vals[i])        for b in valid_bins for i in b['indices']]\n",
    "\n",
    "        print(\"\\nBin Analysis Summary\")\n",
    "        print(f\"  Number of valid IS-bins   : {len(valid_bins)}\")\n",
    "        print(f\"  IS-bins range             : [{min(bin_centers):.3f}, {max(bin_centers):.3f}]\")\n",
    "        print(f\"  Complexity score range    : [{min(all_complexities):.3f}, {max(all_complexities):.3f}]\")\n",
    "        print(f\"  σ range                   : [{min(all_sigmas):.3f}, {max(all_sigmas):.3f}]\")\n",
    "    else:\n",
    "        print(\"\\nBin Analysis Summary\")\n",
    "        print(\"  No valid IS bins — cannot compute summary ranges.\")\n",
    "\n",
    "\n",
    "    # ───────────────────────────── Per-Bin Table ─────────────────────────────\n",
    "    # Build a lookup from bin_id → mean input_sens-info\n",
    "    bin_centers = {\n",
    "        b['bin_id']: np.mean([input_sens_means[i] for i in b['indices']])\n",
    "        for b in valid_bins\n",
    "    }\n",
    "\n",
    "    print(\"\\nPer-Bin Table (one row per valid bin)\")\n",
    "    print(\"|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\")\n",
    "    print(\"|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\")\n",
    "\n",
    "    for _, row in per_bin_metrics.iterrows():\n",
    "        center = bin_centers[row['IS_bin']]\n",
    "        print(\n",
    "            f\"| {center:6.2f} | \"\n",
    "            f\"{int(row['#simple']):6d} | \"\n",
    "            f\"{int(row['#complex']):7d} | \"\n",
    "            f\"{row['CV']:7.3f} | \"\n",
    "            f\"{row['ratio']:7.3f} | \"\n",
    "            f\"{row['pval_ratio']:10.3f} | \"\n",
    "            f\"{row['MI_ACS']:8.3f} | \"\n",
    "            f\"{row['MI_pval_ACS']:8.3f} | \"\n",
    "            f\"{row['MI_all_params']:9.3f} | \"\n",
    "            f\"{row['MI_pval_all_params']:11.3f} |\"\n",
    "        )\n",
    "\n",
    "        # then downstream you can still compute run-level statistics:\n",
    "        mean_cv      = per_bin_metrics['CV'].mean()\n",
    "        max_cv       = per_bin_metrics['CV'].max()\n",
    "        mean_mi_acs  = per_bin_metrics['MI_ACS'].mean()\n",
    "        median_mi_acs= per_bin_metrics['MI_ACS'].median()\n",
    "        max_mi_acs   = per_bin_metrics['MI_ACS'].max()\n",
    "        \n",
    "    print(\"\\nRun-Level Metrics Table\")\n",
    "    if per_bin_metrics.empty:\n",
    "        print(\"No valid run-level metrics due to empty bins.\")\n",
    "    else:\n",
    "        mean_mi_params   = per_bin_metrics['MI_all_params'].mean()\n",
    "        median_mi_params = per_bin_metrics['MI_all_params'].median()\n",
    "        max_mi_params    = per_bin_metrics['MI_all_params'].max()\n",
    "        \n",
    "    # --- prepare your statistics ---\n",
    "    std_ratio = np.nanstd(ratios)\n",
    "\n",
    "    # --- table formatters ---\n",
    "    header_fmt = \"| {0:35s} | {1:43s} |\"\n",
    "    row_fmt    = \"| {0:35s} | {1:43s} |\"\n",
    "\n",
    "    # print the table\n",
    "    print(header_fmt.format(\"Metric\", \"Value\"))\n",
    "    # separator line: 37 dashes for the first column, 45 for the second\n",
    "    print(\"|\" + \"-\"*37 + \"|\" + \"-\"*45 + \"|\")\n",
    "\n",
    "    # each row uses the same row_fmt\n",
    "    print(row_fmt.format(\n",
    "        \"Mean CV(bin) ± SD\",\n",
    "        f\"{mean_cv:.3f} ± {per_bin_metrics['CV'].std():.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"Max CV(bin)\",\n",
    "        f\"{max_cv:.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"Ratio σ(simple/complex) ± SD\",\n",
    "        f\"{mean_ratio:.3f} ± {std_ratio:.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"KS p-value σ(simple/complex)\",\n",
    "        f\"{median_p:.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI ACS mean ± SD\",\n",
    "        f\"{mean_mi_acs:.3f} ± {per_bin_metrics['MI_ACS'].std():.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI ACS median (IQR)\",\n",
    "        f\"{median_mi_acs:.3f} [{per_bin_metrics['MI_ACS'].quantile(0.25):.3f},{per_bin_metrics['MI_ACS'].quantile(0.75):.3f}]\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI ACS max\",\n",
    "        f\"{max_mi_acs:.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI all_params mean ± SD\",\n",
    "        f\"{mean_mi_params:.3f} ± {per_bin_metrics['MI_all_params'].std():.3f}\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI all_params median (IQR)\",\n",
    "        f\"{median_mi_params:.3f} [{per_bin_metrics['MI_all_params'].quantile(0.25):.3f},{per_bin_metrics['MI_all_params'].quantile(0.75):.3f}]\"\n",
    "    ))\n",
    "    print(row_fmt.format(\n",
    "        \"MI all_params max\",\n",
    "        f\"{max_mi_params:.3f}\"\n",
    "    ))\n",
    "        \n",
    "\n",
    "    # After printing Run-Level Metrics:\n",
    "    if per_bin_metrics.empty:\n",
    "        mean_cv = max_cv = mean_mi_acs = median_mi_acs = max_mi_acs = np.nan\n",
    "    else:\n",
    "        mean_cv = per_bin_metrics['CV'].mean()\n",
    "        max_cv  = per_bin_metrics['CV'].max()\n",
    "        mean_mi_acs   = per_bin_metrics['MI_ACS'].mean()\n",
    "        median_mi_acs = per_bin_metrics['MI_ACS'].median()\n",
    "        max_mi_acs    = per_bin_metrics['MI_ACS'].max()\n",
    "    \n",
    "    # Stability metrics unchanged\n",
    "    ratios = (np.array(lambda_stds) ** 2) * (np.array(model_depths) - 1)\n",
    "    mean_sigma, std_sigma = np.mean(ratios), np.std(ratios)\n",
    "    cv_sigma = std_sigma / (mean_sigma + 1e-12)\n",
    "    \n",
    "    # ───────── RUN-LEVEL STABILITY PRINT ─────────\n",
    "    print(\"\\nRun-level Geometric Stability\")\n",
    "    print(f\"  Mean σ²×(L-1): {mean_sigma:.3e}\")\n",
    "    print(f\"  Std  σ²×(L-1): {std_sigma:.3e}\")\n",
    "    print(f\"  CV   σ²×(L-1): {cv_sigma:.3f}\")\n",
    "    \n",
    "    # ────────── PLOTS ──────────────────────\n",
    "    if cfg.get('make_plots', False) and (grp is not None):\n",
    "        inv_Lminus1 = 1.0 / (np.array(model_depths) - 1)\n",
    "        sigma2      = np.array(lambda_stds)**2\n",
    "        plot_key_results(\n",
    "            {\n",
    "                'grouped':     grp, \n",
    "                'inv_Lminus1': inv_Lminus1,\n",
    "                'sigma2':      sigma2\n",
    "            },\n",
    "            title_suffix = f\" ({cfg['run_id']})\"\n",
    "        )\n",
    "    # ────────────────────────────────────────\n",
    "\n",
    "    return {\n",
    "        'run_id': cfg['run_id'],\n",
    "        'architecture': cfg['architecture'],\n",
    "        'dataset': cfg['dataset'],\n",
    "        'n_models_accepted': len(lambda_stds),\n",
    "        'n_models_used_in_bins': sum(len(b['indices']) for b in valid_bins),\n",
    "        'n_lambda_vals': len(lambda_flat),\n",
    "        'n_input_sens_vals': len(input_sens_flat),\n",
    "        'n_IS_bins_attempted': cfg['n_variance_levels'],\n",
    "        'n_IS_bins_valid': len(valid_bins),\n",
    "        'input_sens_depth_slope': slope_input_sens,\n",
    "        'input_sens_depth_r2': r_value_input_sens**2 if not np.isnan(r_value_input_sens) else np.nan,\n",
    "        'mean_sigma_ratio_simple_to_complex': mean_ratio,\n",
    "        'ks_pvalue_sigma_ratio':             median_p,\n",
    "        'cv_bins_mean': mean_cv,\n",
    "        'cv_bins_std':  np.nan if per_bin_metrics.empty else per_bin_metrics['CV'].std(),\n",
    "        'cv_bins_max':  max_cv,\n",
    "        'mean_mi_acs': mean_mi_acs,\n",
    "        'median_mi_acs': median_mi_acs,\n",
    "        'max_mi_acs': max_mi_acs,\n",
    "        'mean_mi_all_params':    mean_mi_params,\n",
    "        'median_mi_all_params':  median_mi_params,\n",
    "        'max_mi_all_params':     max_mi_params,\n",
    "        'mean_sigma2_ratio': mean_sigma,\n",
    "        'std_sigma2_ratio': std_sigma,\n",
    "        'cv_sigma2_ratio': cv_sigma\n",
    "    }\n",
    "\n",
    "def analyze_experiment_results(results_list):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Helper to compute mean ± 95% CI\n",
    "    def mean_ci(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) < 2:\n",
    "            return None\n",
    "        m = a.mean()\n",
    "        se = stats.sem(a)\n",
    "        h = stats.t.ppf(0.975, len(a)-1) * se\n",
    "        return m, m-h, m+h\n",
    "\n",
    "    # Format an array as mean ± half-width (95% CI, n=…)\n",
    "    def fmt_ci(arr):\n",
    "        ci = mean_ci(arr)\n",
    "        cnt = np.count_nonzero(~np.isnan(arr))\n",
    "        if ci is None:\n",
    "            return \"n/a\"\n",
    "        m, lo, hi = ci\n",
    "        return f\"{m:.3f} ± {(m-lo):.3f} (95% CI, n={cnt})\"\n",
    "\n",
    "    # Format p-values as median [IQR] (n=…)\n",
    "    def fmt_p(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) == 0:\n",
    "            return \"n/a\"\n",
    "        q25, q75 = np.percentile(a, [25, 75])\n",
    "        return f\"{np.median(a):.3f} [IQR {q25:.3f}, {q75:.3f}] (n={len(a)})\"\n",
    "\n",
    "    # Simple mean±SD\n",
    "    def fmt_sd(arr):\n",
    "        a = np.array(arr, float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        if len(a) == 0:\n",
    "            return \"n/a\"\n",
    "        return f\"{a.mean():.3f} ± {a.std():.3f}\"\n",
    "\n",
    "    n_runs = len(results_list)\n",
    "\n",
    "    # collect arrays across all runs, using .get to default to nan\n",
    "    ratio_arr   = [r.get('mean_sigma_ratio_simple_to_complex',   np.nan) for r in results_list]\n",
    "    pval_arr    = [r.get('ks_pvalue_sigma_ratio',               np.nan) for r in results_list]\n",
    "    cv_arr      = [r.get('cv_bins_mean',                        np.nan) for r in results_list]\n",
    "    cvmax_arr   = [r.get('cv_bins_max',                         np.nan) for r in results_list]\n",
    "    mi_acs_arr  = [r.get('mean_mi_acs',                         np.nan) for r in results_list]\n",
    "    mi_ap_arr   = [r.get('mean_mi_all_params',                  np.nan) for r in results_list]\n",
    "    s2_m_arr    = [r.get('mean_sigma2_ratio',                   np.nan) for r in results_list]\n",
    "    s2_s_arr    = [r.get('std_sigma2_ratio',                    np.nan) for r in results_list]\n",
    "    s2_cv_arr   = [r.get('cv_sigma2_ratio',                     np.nan) for r in results_list]\n",
    "\n",
    "    # --- CROSS-RUN SUMMARY ---\n",
    "    print(\"\\n===== CROSS-RUN SUMMARY =====\\n\")\n",
    "    # Metadata\n",
    "    tm = sum(r.get('n_models_accepted',0) for r in results_list)\n",
    "    tl = sum(r.get('n_lambda_vals',0)       for r in results_list)\n",
    "    tf = sum(r.get('n_input_sens_vals',0)       for r in results_list)\n",
    "    ba = sum(r.get('n_IS_bins_attempted',0) for r in results_list)\n",
    "    bv = sum(r.get('n_IS_bins_valid',0)     for r in results_list)\n",
    "    ub = np.nanmean([r.get('n_models_used_in_bins', np.nan) for r in results_list])\n",
    "    print(\"Metadata:\")\n",
    "    print(f\"  Total models accepted          : {tm}\")\n",
    "    print(f\"  Total λᵢ values                : {tl}\")\n",
    "    print(f\"  Total IS values            : {tf}\")\n",
    "    print(f\"  Valid IS bins                  : {bv}\")\n",
    "    print(f\"  Total IS bins attempted        : {ba}\")\n",
    "    print(f\"  Avg models per run             : {tm/n_runs:.2f}\")\n",
    "    print(f\"  Avg λᵢ per model               : {tl/tm:.2f}\")\n",
    "    print(f\"  Avg models used-in-bins per run: {ub:.2f}\\n\")\n",
    "\n",
    "    # 1. Per-IS-bin σ Variation\n",
    "    print(\"1. Per-IS-bin σ Variation:\")\n",
    "    print(f\"   Ratio σ(simple/complex)  : {fmt_ci(ratio_arr)}\")\n",
    "    print(f\"   KS-p-value summary       : {fmt_p(pval_arr)}\")\n",
    "    print(f\"   Runs with p<0.05         : {int(np.sum(np.array(pval_arr)<0.05))} / {n_runs}\\n\")\n",
    "\n",
    "    # 2. Per-IS-bin CV(σ)\n",
    "    print(\"2. Per-IS-bin CV(σ):\")\n",
    "    print(f\"   Mean CV(bin) : {fmt_ci(cv_arr)}\")\n",
    "    print(f\"   Max  CV(bin) : {fmt_ci(cvmax_arr)}\\n\")\n",
    "\n",
    "    # 3. Mutual Information ACS\n",
    "    print(\"3. Mutual Information ACS:\")\n",
    "    if all(np.isnan(mi_acs_arr)):\n",
    "        print(\"   MI_ACS: n/a\\n\")\n",
    "    else:\n",
    "        print(f\"   Mean ± SD     : {fmt_sd(mi_acs_arr)}\")\n",
    "        med = np.nanmedian(mi_acs_arr)\n",
    "        iqr = (np.nanpercentile(mi_acs_arr,25), np.nanpercentile(mi_acs_arr,75))\n",
    "        print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "        print(f\"   Max           : {np.nanmax(mi_acs_arr):.3f}\\n\")\n",
    "\n",
    "    # 4. Mutual Information all_params\n",
    "    print(\"4. Mutual Information all_params:\")\n",
    "    if all(np.isnan(mi_ap_arr)):\n",
    "        print(\"   MI_all_params: n/a\\n\")\n",
    "    else:\n",
    "        print(f\"   Mean ± SD     : {fmt_sd(mi_ap_arr)}\")\n",
    "        med = np.nanmedian(mi_ap_arr)\n",
    "        iqr = (np.nanpercentile(mi_ap_arr,25), np.nanpercentile(mi_ap_arr,75))\n",
    "        print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "        print(f\"   Max           : {np.nanmax(mi_ap_arr):.3f}\\n\")\n",
    "\n",
    "    # 5. Stability σ²×(L-1)\n",
    "    print(\"5. Stability σ²×(L-1):\")\n",
    "    print(f\"   Mean : {fmt_ci(s2_m_arr)}\")\n",
    "    print(f\"   Std  : {fmt_ci(s2_s_arr)}\")\n",
    "    print(f\"   CV   : {fmt_ci(s2_cv_arr)}\\n\")\n",
    "\n",
    "    # --- PER-ARCH/DATASET BREAKDOWN ---\n",
    "    print(\"\\n===== PER-ARCH/DATASET BREAKDOWN =====\\n\")\n",
    "    by_group = defaultdict(list)\n",
    "    for r in results_list:\n",
    "        by_group[(r.get('architecture','?'), r.get('dataset','?'))].append(r)\n",
    "\n",
    "    for (arch, ds), grp in by_group.items():\n",
    "        print(f\"--- {arch.upper()} / {ds.upper()} (n={len(grp)}) ---\")\n",
    "        # collect per-group arrays\n",
    "        g_ratio = [g.get('mean_sigma_ratio_simple_to_complex', np.nan) for g in grp]\n",
    "        g_pval  = [g.get('ks_pvalue_sigma_ratio',             np.nan) for g in grp]\n",
    "        g_cv    = [g.get('cv_bins_mean',                       np.nan) for g in grp]\n",
    "        g_cvmax = [g.get('cv_bins_max',                        np.nan) for g in grp]\n",
    "        g_mi_acs= [g.get('mean_mi_acs',                        np.nan) for g in grp]\n",
    "        g_mi_ap = [g.get('mean_mi_all_params',                 np.nan) for g in grp]\n",
    "        g_s2_m  = [g.get('mean_sigma2_ratio',                  np.nan) for g in grp]\n",
    "        g_s2_s  = [g.get('std_sigma2_ratio',                   np.nan) for g in grp]\n",
    "        g_s2_cv = [g.get('cv_sigma2_ratio',                    np.nan) for g in grp]\n",
    "\n",
    "        # 1.\n",
    "        print(\"1. Per-IS-bin σ Variation:\")\n",
    "        if len(grp) == 1:\n",
    "            print(f\"   Ratio σ(simple/complex)  : {g_ratio[0]:.3f}\")\n",
    "            print(f\"   KS-p-value summary       : {g_pval[0]:.3f}\")\n",
    "            print(f\"   Runs with p<0.05         : {int(g_pval[0]<0.05)} / 1\\n\")\n",
    "        else:\n",
    "            print(f\"   Ratio σ(simple/complex)  : {fmt_ci(g_ratio)}\")\n",
    "            print(f\"   KS-p-value summary       : {fmt_p(g_pval)}\")\n",
    "            print(f\"   Runs with p<0.05         : {int(np.sum(np.array(g_pval)<0.05))} / {len(grp)}\\n\")\n",
    "\n",
    "        # 2.\n",
    "        print(\"2. Per-IS-bin CV(σ):\")\n",
    "        print(f\"   Mean CV(bin) : {fmt_ci(g_cv)}\")\n",
    "        print(f\"   Max  CV(bin) : {fmt_ci(g_cvmax)}\\n\")\n",
    "\n",
    "        # 3.\n",
    "        print(\"3. Mutual Information ACS:\")\n",
    "        if all(np.isnan(g_mi_acs)):\n",
    "            print(\"   MI_ACS: n/a\\n\")\n",
    "        else:\n",
    "            print(f\"   Mean ± SD     : {fmt_sd(g_mi_acs)}\")\n",
    "            med = np.nanmedian(g_mi_acs)\n",
    "            iqr = (np.nanpercentile(g_mi_acs,25), np.nanpercentile(g_mi_acs,75))\n",
    "            print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "            print(f\"   Max           : {np.nanmax(g_mi_acs):.3f}\\n\")\n",
    "\n",
    "        # 4.\n",
    "        print(\"4. Mutual Information all_params:\")\n",
    "        if all(np.isnan(g_mi_ap)):\n",
    "            print(\"   MI_all_params: n/a\\n\")\n",
    "        else:\n",
    "            print(f\"   Mean ± SD     : {fmt_sd(g_mi_ap)}\")\n",
    "            med = np.nanmedian(g_mi_ap)\n",
    "            iqr = (np.nanpercentile(g_mi_ap,25), np.nanpercentile(g_mi_ap,75))\n",
    "            print(f\"   Median [IQR]  : {med:.3f} [{iqr[0]:.3f}–{iqr[1]:.3f}]\")\n",
    "            print(f\"   Max           : {np.nanmax(g_mi_ap):.3f}\\n\")\n",
    "\n",
    "        # 5.\n",
    "        print(\"5. Stability σ²×(L-1):\")\n",
    "        print(f\"   Mean : {fmt_ci(g_s2_m)}\")\n",
    "        print(f\"   Std  : {fmt_ci(g_s2_s)}\")\n",
    "        print(f\"   CV   : {fmt_ci(g_s2_cv)}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"overall\": {\n",
    "            \"ratio_CI\": fmt_ci(ratio_arr),\n",
    "            \"ks_p\":     fmt_p(pval_arr),\n",
    "            \"cv_mean\":  fmt_ci(cv_arr),\n",
    "            \"cv_max\":   fmt_ci(cvmax_arr),\n",
    "            \"mi_acs\":   fmt_sd(mi_acs_arr) if not all(np.isnan(mi_acs_arr)) else \"n/a\",\n",
    "            \"mi_ap\":    fmt_sd(mi_ap_arr)  if not all(np.isnan(mi_ap_arr))  else \"n/a\",\n",
    "            \"s2_mean\":  fmt_ci(s2_m_arr),\n",
    "            \"s2_std\":   fmt_ci(s2_s_arr),\n",
    "            \"s2_cv\":    fmt_ci(s2_cv_arr),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_statistical_consistency_experiments(\n",
    "    seeds,\n",
    "    architectures,\n",
    "    datasets,\n",
    "    data_fraction,\n",
    "    noise_levels=(0.1, 0.2),\n",
    "    n_models=100,\n",
    "    complexity_percentile=10,\n",
    "    n_variance_levels=20,\n",
    "    n_bootstrap=50,\n",
    "    make_plots=False,\n",
    "    architecture_configs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs experiments over all combinations of seed, architecture, dataset and noise level.\n",
    "    Each call to run_experiment gets a single scalar noise_std.\n",
    "    \"\"\"\n",
    "    total = len(seeds) * len(architectures) * len(datasets) * len(noise_levels)\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        for arch in architectures:\n",
    "            for ds in datasets:\n",
    "                for noise in noise_levels:\n",
    "                    count += 1\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"Experiment {count}/{total}: \"\n",
    "                          f\"arch={arch}, dataset={ds}, seed={seed}, noise={noise}\")\n",
    "\n",
    "                    # build config for this run\n",
    "                    cfg = {\n",
    "                        'seed': seed,\n",
    "                        'architecture': arch,\n",
    "                        'dataset': ds,\n",
    "                        'data_fraction': data_fraction,\n",
    "                        'noise_std': noise,\n",
    "                        'n_models': n_models,\n",
    "                        'complexity_percentile': complexity_percentile,\n",
    "                        'n_variance_levels': n_variance_levels,\n",
    "                        'n_bootstrap': n_bootstrap,\n",
    "                        'make_plots': make_plots,\n",
    "                        # unique run_id\n",
    "                        'run_id': f\"{arch}_{ds}_s{seed}_n{noise}\",\n",
    "                        # bring in architecture-specific settings\n",
    "                        **architecture_configs[arch]\n",
    "                    }\n",
    "\n",
    "                    # run it!\n",
    "                    res = run_experiment(cfg)\n",
    "                    all_results.append(res)\n",
    "\n",
    "    # once all runs are done, aggregate\n",
    "    analysis = analyze_experiment_results(all_results)\n",
    "    return all_results, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370208b-6a58-4231-9db0-61ed4c05d926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T09:05:03.166315Z",
     "iopub.status.busy": "2025-06-09T09:05:03.165584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment 1/36: arch=cnn, dataset=cifar, seed=1, noise=0.1\n",
      "Running run_id       : cnn_cifar_s1_n0.1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Files already downloaded and verified\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:52<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2834\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.295   (R² = 0.628)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 2\n",
      "  IS-bins range             : [68.870, 351.859]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.160, 2.231]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  68.87 |     21 |      18 |   0.335 |   1.125 |      0.604 |    0.000 |    1.000 |     0.000 |       1.000 |\n",
      "| 351.86 |      3 |       3 |   0.507 |   2.085 |      0.600 |    0.000 |    1.000 |     0.000 |       1.000 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.421 ± 0.121                               |\n",
      "| Max CV(bin)                         | 0.507                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.605 ± 0.480                               |\n",
      "| KS p-value σ(simple/complex)        | 0.602                                       |\n",
      "| MI ACS mean ± SD                    | 0.000 ± 0.000                               |\n",
      "| MI ACS median (IQR)                 | 0.000 [0.000,0.000]                         |\n",
      "| MI ACS max                          | 0.000                                       |\n",
      "| MI all_params mean ± SD             | 0.000 ± 0.000                               |\n",
      "| MI all_params median (IQR)          | 0.000 [0.000,0.000]                         |\n",
      "| MI all_params max                   | 0.000                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.864e+01\n",
      "  Std  σ²×(L-1): 2.931e+01\n",
      "  CV   σ²×(L-1): 0.759\n",
      "\n",
      "============================================================\n",
      "Experiment 2/36: arch=cnn, dataset=cifar, seed=1, noise=0.25\n",
      "Running run_id       : cnn_cifar_s1_n0.25\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:38<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2831\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.295   (R² = 0.628)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 2\n",
      "  IS-bins range             : [68.870, 351.859]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.200, 2.168]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  68.87 |     21 |      18 |   0.321 |   1.119 |      0.604 |    0.000 |    1.000 |     0.000 |       1.000 |\n",
      "| 351.86 |      3 |       3 |   0.543 |   1.935 |      0.600 |    0.017 |    0.408 |     0.078 |       0.214 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.432 ± 0.157                               |\n",
      "| Max CV(bin)                         | 0.543                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.527 ± 0.408                               |\n",
      "| KS p-value σ(simple/complex)        | 0.602                                       |\n",
      "| MI ACS mean ± SD                    | 0.009 ± 0.012                               |\n",
      "| MI ACS median (IQR)                 | 0.009 [0.004,0.013]                         |\n",
      "| MI ACS max                          | 0.017                                       |\n",
      "| MI all_params mean ± SD             | 0.039 ± 0.055                               |\n",
      "| MI all_params median (IQR)          | 0.039 [0.019,0.058]                         |\n",
      "| MI all_params max                   | 0.078                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.831e+01\n",
      "  Std  σ²×(L-1): 3.029e+01\n",
      "  CV   σ²×(L-1): 0.791\n",
      "\n",
      "============================================================\n",
      "Experiment 3/36: arch=cnn, dataset=cifar, seed=1, noise=0.5\n",
      "Running run_id       : cnn_cifar_s1_n0.5\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:41<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2836\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.295   (R² = 0.628)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 2\n",
      "  IS-bins range             : [68.870, 351.859]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.242, 2.162]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  68.87 |     21 |      18 |   0.309 |   1.096 |      0.473 |    0.000 |    1.000 |     0.000 |       1.000 |\n",
      "| 351.86 |      3 |       3 |   0.573 |   1.818 |      0.600 |    0.079 |    0.299 |     0.089 |       0.184 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.441 ± 0.187                               |\n",
      "| Max CV(bin)                         | 0.573                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.457 ± 0.361                               |\n",
      "| KS p-value σ(simple/complex)        | 0.536                                       |\n",
      "| MI ACS mean ± SD                    | 0.039 ± 0.056                               |\n",
      "| MI ACS median (IQR)                 | 0.039 [0.020,0.059]                         |\n",
      "| MI ACS max                          | 0.079                                       |\n",
      "| MI all_params mean ± SD             | 0.045 ± 0.063                               |\n",
      "| MI all_params median (IQR)          | 0.045 [0.022,0.067]                         |\n",
      "| MI all_params max                   | 0.089                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.886e+01\n",
      "  Std  σ²×(L-1): 3.135e+01\n",
      "  CV   σ²×(L-1): 0.807\n",
      "\n",
      "============================================================\n",
      "Experiment 4/36: arch=cnn, dataset=mnist, seed=1, noise=0.1\n",
      "Running run_id       : cnn_mnist_s1_n0.1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 39\n",
      "Input shape          : (8, 1, 39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:47<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2821\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.341   (R² = 0.631)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [70.838, 70.838]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.184, 2.285]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  70.84 |     21 |      20 |   0.346 |   1.163 |      0.410 |    0.200 |    0.040 |     0.000 |       1.000 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.346 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.346                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.163 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.410                                       |\n",
      "| MI ACS mean ± SD                    | 0.200 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.200 [0.200,0.200]                         |\n",
      "| MI ACS max                          | 0.200                                       |\n",
      "| MI all_params mean ± SD             | 0.000 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.000 [0.000,0.000]                         |\n",
      "| MI all_params max                   | 0.000                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.746e+01\n",
      "  Std  σ²×(L-1): 2.787e+01\n",
      "  CV   σ²×(L-1): 0.744\n",
      "\n",
      "============================================================\n",
      "Experiment 5/36: arch=cnn, dataset=mnist, seed=1, noise=0.25\n",
      "Running run_id       : cnn_mnist_s1_n0.25\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 39\n",
      "Input shape          : (8, 1, 39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:45<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2829\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.341   (R² = 0.631)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [70.838, 70.838]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.219, 2.107]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  70.84 |     21 |      20 |   0.332 |   1.130 |      0.450 |    0.041 |    0.328 |     0.000 |       1.000 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.332 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.332                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.130 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.450                                       |\n",
      "| MI ACS mean ± SD                    | 0.041 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.041 [0.041,0.041]                         |\n",
      "| MI ACS max                          | 0.041                                       |\n",
      "| MI all_params mean ± SD             | 0.000 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.000 [0.000,0.000]                         |\n",
      "| MI all_params max                   | 0.000                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.711e+01\n",
      "  Std  σ²×(L-1): 2.910e+01\n",
      "  CV   σ²×(L-1): 0.784\n",
      "\n",
      "============================================================\n",
      "Experiment 6/36: arch=cnn, dataset=mnist, seed=1, noise=0.5\n",
      "Running run_id       : cnn_mnist_s1_n0.5\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 39\n",
      "Input shape          : (8, 1, 39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [03:41<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 2829\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-4.341   (R² = 0.631)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [70.838, 70.838]\n",
      "  Complexity score range    : [-0.715, 0.742]\n",
      "  σ range                   : [0.290, 1.983]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "|  70.84 |     21 |      20 |   0.320 |   1.101 |      0.466 |    0.035 |    0.338 |     0.000 |       0.383 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.320 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.320                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.101 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.466                                       |\n",
      "| MI ACS mean ± SD                    | 0.035 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.035 [0.035,0.035]                         |\n",
      "| MI ACS max                          | 0.035                                       |\n",
      "| MI all_params mean ± SD             | 0.000 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.000 [0.000,0.000]                         |\n",
      "| MI all_params max                   | 0.000                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 3.800e+01\n",
      "  Std  σ²×(L-1): 3.036e+01\n",
      "  CV   σ²×(L-1): 0.799\n",
      "\n",
      "============================================================\n",
      "Experiment 7/36: arch=resnet, dataset=cifar, seed=1, noise=0.1\n",
      "Running run_id       : resnet_cifar_s1_n0.1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [10:36<00:00,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 3283\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-5.338   (R² = 0.726)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [107.637, 107.637]\n",
      "  Complexity score range    : [-0.615, 1.213]\n",
      "  σ range                   : [0.510, 1.489]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "| 107.64 |     16 |      16 |   0.236 |   1.039 |      0.952 |    0.052 |    0.264 |     0.087 |       0.040 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.236 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.236                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.039 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.952                                       |\n",
      "| MI ACS mean ± SD                    | 0.052 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.052 [0.052,0.052]                         |\n",
      "| MI ACS max                          | 0.052                                       |\n",
      "| MI all_params mean ± SD             | 0.087 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.087 [0.087,0.087]                         |\n",
      "| MI all_params max                   | 0.087                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 2.234e+01\n",
      "  Std  σ²×(L-1): 1.808e+01\n",
      "  CV   σ²×(L-1): 0.809\n",
      "\n",
      "============================================================\n",
      "Experiment 8/36: arch=resnet, dataset=cifar, seed=1, noise=0.25\n",
      "Running run_id       : resnet_cifar_s1_n0.25\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [11:16<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 3283\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-5.338   (R² = 0.726)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [107.637, 107.637]\n",
      "  Complexity score range    : [-0.615, 1.213]\n",
      "  σ range                   : [0.515, 1.483]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "| 107.64 |     16 |      16 |   0.232 |   1.052 |      0.952 |    0.000 |    1.000 |     0.058 |       0.075 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.232 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.232                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.052 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.952                                       |\n",
      "| MI ACS mean ± SD                    | 0.000 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.000 [0.000,0.000]                         |\n",
      "| MI ACS max                          | 0.000                                       |\n",
      "| MI all_params mean ± SD             | 0.058 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.058 [0.058,0.058]                         |\n",
      "| MI all_params max                   | 0.058                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 2.266e+01\n",
      "  Std  σ²×(L-1): 1.851e+01\n",
      "  CV   σ²×(L-1): 0.817\n",
      "\n",
      "============================================================\n",
      "Experiment 9/36: arch=resnet, dataset=cifar, seed=1, noise=0.5\n",
      "Running run_id       : resnet_cifar_s1_n0.5\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 44\n",
      "Input shape          : (8, 3, 44, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models: 100%|██████████| 100/100 [11:15<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models accepted       : 100\n",
      "Unique datapoints               : 3282\n",
      "\n",
      "IS Scaling with Depth\n",
      "  IS   ~ depth^-5.338   (R² = 0.726)\n",
      "\n",
      "Bin Analysis Summary\n",
      "  Number of valid IS-bins   : 1\n",
      "  IS-bins range             : [107.637, 107.637]\n",
      "  Complexity score range    : [-0.615, 1.213]\n",
      "  σ range                   : [0.531, 1.476]\n",
      "\n",
      "Per-Bin Table (one row per valid bin)\n",
      "|   IS   | simple | complex |   CV    | Ratio σ | pval_ratio |  MI_ACS  | pval_ACS | MI_params | pval_params |\n",
      "|--------|--------|---------|---------|---------|------------|----------|----------|-----------|-------------|\n",
      "| 107.64 |     16 |      16 |   0.226 |   1.065 |      0.716 |    0.002 |    0.458 |     0.036 |       0.169 |\n",
      "\n",
      "Run-Level Metrics Table\n",
      "| Metric                              | Value                                       |\n",
      "|-------------------------------------|---------------------------------------------|\n",
      "| Mean CV(bin) ± SD                   | 0.226 ± nan                                 |\n",
      "| Max CV(bin)                         | 0.226                                       |\n",
      "| Ratio σ(simple/complex) ± SD        | 1.065 ± 0.000                               |\n",
      "| KS p-value σ(simple/complex)        | 0.716                                       |\n",
      "| MI ACS mean ± SD                    | 0.002 ± nan                                 |\n",
      "| MI ACS median (IQR)                 | 0.002 [0.002,0.002]                         |\n",
      "| MI ACS max                          | 0.002                                       |\n",
      "| MI all_params mean ± SD             | 0.036 ± nan                                 |\n",
      "| MI all_params median (IQR)          | 0.036 [0.036,0.036]                         |\n",
      "| MI all_params max                   | 0.036                                       |\n",
      "\n",
      "Run-level Geometric Stability\n",
      "  Mean σ²×(L-1): 2.308e+01\n",
      "  Std  σ²×(L-1): 1.904e+01\n",
      "  CV   σ²×(L-1): 0.825\n",
      "\n",
      "============================================================\n",
      "Experiment 10/36: arch=resnet, dataset=mnist, seed=1, noise=0.1\n",
      "Running run_id       : resnet_mnist_s1_n0.1\n",
      "\n",
      "Resize scale factor  : 1.40  →  target size 39\n",
      "Input shape          : (8, 1, 39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring models:  47%|████▋     | 47/100 [06:11<01:50,  2.08s/it]"
     ]
    }
   ],
   "source": [
    "# architecture_configs\n",
    "architecture_configs = {\n",
    "    'cnn': {\n",
    "        'depth_choices': [4, 6, 8, 10, 20, 40, 80, 100],\n",
    "        'resize_range': (1.0, 4.0),\n",
    "        'param_ranges': {\n",
    "            'strides':  [1, 2, 3],\n",
    "            'kernels':  [3, 5, 7, 9, 11, 13, 15, 17],\n",
    "            'channels': [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "        }\n",
    "    },\n",
    "    'resnet': {\n",
    "        'depth_choices': [4, 6, 8, 10, 20, 40, 80, 100],\n",
    "        'resize_range': (1.0, 4.0),\n",
    "        'param_ranges': {\n",
    "            'channels':          [16, 32, 64, 128, 256, 512, 1024],\n",
    "            'block_sizes':       [1, 2, 3, 4, 5],\n",
    "            'kernel_sizes':      [1, 3, 5, 7, 9],\n",
    "            'strides':           [1, 2, 3],\n",
    "            'bottleneck_ratios': [1, 0.75, 0.5, 0.25, 0.125],\n",
    "            'projection_types':  ['identity', 'conv1x1'],\n",
    "            'activation_functions': ['relu', 'leaky_relu']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# call to main script:\n",
    "results, analysis = run_statistical_consistency_experiments(\n",
    "    seeds=[1, 12, 123],\n",
    "    architectures=['cnn', 'resnet'],\n",
    "    datasets=['cifar', 'mnist'],\n",
    "    data_fraction=0.0008,          # proportion of test set to sample\n",
    "    noise_levels=[0.1, 0.25, 0.5],  # noise std devs to sweep over\n",
    "    n_models=100,                  # how many random nets per run\n",
    "    complexity_percentile=25,      # top/bottom 25% are “complex”/“simple”\n",
    "    n_variance_levels=20,          # how many IS‐bins to attempt\n",
    "    n_bootstrap=20,\n",
    "    make_plots=False,               # toggle your two matplotlib plots\n",
    "    architecture_configs=architecture_configs\n",
    ")\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc30076-72d0-4a11-93ca-763b263c1ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
